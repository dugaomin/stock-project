# 🌐 全网筛选逻辑详解与加速方案

## 📋 筛选逻辑详解

### 整体流程

```
获取股票列表（5274只，排除ST股）
    ↓
对每只股票执行分析（并发处理）
    ↓
【第一层筛选】基本面检查
    ├─ 审计意见：近5年全部为"标准无保留意见"
    └─ 现金流质量：近5年经营现金流全部≥0
    ↓
【第二层筛选】估值检查
    ├─ 修正市赚率（PR）≤ 设定阈值（默认1.0）
    └─ 加权ROE ≥ 设定最小值（默认10.0%）
    ↓
收集通过筛选的股票
    ↓
按修正PR从低到高排序
    ↓
输出结果列表
```

---

## 🔍 详细筛选规则

### 第一层：基本面筛选（必须全部满足）

#### 1. 审计意见检查
```python
# 检查近5年的审计意见
recent_audits = audit_records[:5]  # 取最新的5条记录
all_standard = all(record.is_standard for record in recent_audits)
# 必须全部为"标准无保留意见"
```

**业务逻辑**：
- 获取最近5年的审计记录
- 检查每条记录是否为"标准无保留意见"
- 必须全部通过，否则淘汰

#### 2. 现金流质量检查
```python
# 检查近5年的经营现金流
recent_5_years = metrics.head(5)
all_positive = recent_5_years['cashflow_positive'].all()
# 必须全部≥0
```

**业务逻辑**：
- 获取最近5年的财务数据
- 检查每年经营现金流是否≥0
- 必须全部通过，否则淘汰

**通过条件**：`审计意见通过 AND 现金流全部≥0`

---

### 第二层：估值筛选（必须全部满足）

#### 1. 修正市赚率（PR）检查
```python
# 计算修正市赚率
corrected_pr = N × PE_TTM / ROE / 150
# 其中N是修正系数（根据股息支付率确定）

# 检查是否≤阈值
pr_pass = corrected_pr <= pr_threshold  # 默认1.0
```

**业务逻辑**：
- 获取最新交易日的PE、ROE、分红数据
- 计算修正市赚率（考虑分红质量）
- 必须≤用户设定阈值（默认1.0），否则淘汰

#### 2. ROE检查
```python
# 获取加权ROE
roe_waa = valuation_data.get('roe_waa')

# 检查是否≥最小值
roe_pass = roe_waa >= min_roe  # 默认10.0%
```

**业务逻辑**：
- 获取最新交易日的加权ROE
- 必须≥用户设定最小值（默认10.0%），否则淘汰

**通过条件**：`PR ≤ 阈值 AND ROE ≥ 最小值`

---

### 最终筛选结果

**通过条件**：`基本面通过 AND 估值通过`

**排序规则**：按修正市赚率（PR）从低到高排序（PR越低越便宜，越值得买入）

---

## ⚡ 性能瓶颈分析

### 当前性能情况

从你的截图看到：
- **总股票数**：5274只
- **当前进度**：4/5274 (0.08%)
- **通过数**：0
- **失败数**：4

### 性能瓶颈

#### 1. **API调用次数过多**（主要瓶颈）

每只股票需要调用：
- `analyze_fundamentals()` → **5次API调用**
  - stock_company (公司信息)
  - fina_audit (审计意见)
  - balancesheet (资产负债表)
  - income (利润表)
  - cashflow (现金流量表)
- `check_valuation_pass()` → **3次API调用**
  - daily_basic (PE数据)
  - fina_indicator (ROE数据)
  - dividend (分红数据)

**总计**：每只股票 **8次API调用**

**5274只股票 × 8次 = 42,192次API调用**

#### 2. **API延迟限制**

当前设置：
- `api_delay = 0.5秒`（每次API调用后等待0.5秒）
- `max_workers = 4`（4个并发线程）

**单只股票耗时**：
- 8次API调用 × 0.5秒 = 4秒（不考虑网络延迟）
- 加上网络延迟和数据处理：约 **5-6秒/只**

**总耗时估算**：
- 5274只 × 5秒 = **26,370秒 ≈ 7.3小时**
- 使用4线程并发：**约1.8-2小时**

#### 3. **缓存命中率低**

首次筛选时，缓存命中率为0%，所有数据都需要从API获取。

---

## 🚀 加速方案

### 方案1：提高并发数（最简单，效果明显）

**当前设置**：`max_workers = 4`

**优化建议**：
```python
# 根据你的Tushare积分等级调整
# 中级用户(600-4999分)：每分钟20次 → 可以设置8-10个线程
# 高级用户(5000+分)：每分钟200次 → 可以设置20-30个线程

max_workers = 10  # 从4提升到10
```

**加速效果**：
- 4线程：约2小时
- 10线程：约 **45分钟-1小时**（提升2-2.5倍）

**注意事项**：
- 需要根据你的积分等级调整
- 避免触发API频率限制

---

### 方案2：减少API延迟（需要高积分）

**当前设置**：`api_delay = 0.5秒`

**优化建议**：
```python
# 根据积分等级调整延迟
# 中级用户：0.1-0.2秒
# 高级用户：0秒（无需延迟）

api_delay = 0.1  # 从0.5秒降到0.1秒
```

**加速效果**：
- 0.5秒延迟：单只股票约5秒
- 0.1秒延迟：单只股票约 **1-2秒**（提升2.5-5倍）

**注意事项**：
- 需要足够的积分等级
- 避免触发频率限制

---

### 方案3：利用缓存（最有效）

**策略**：先批量查询一批股票，建立缓存，然后筛选

**步骤**：
1. 第一次筛选：正常执行（建立缓存）
2. 后续筛选：大部分股票使用缓存（<0.1秒）

**加速效果**：
- 首次筛选：2小时（建立缓存）
- 后续筛选：**10-20分钟**（80%+缓存命中率）

**实现方法**：
```python
# 缓存已自动启用，无需额外配置
# 只需确保analyze_fundamentals()的use_cache=True（默认已启用）
```

---

### 方案4：优化筛选顺序（提前淘汰）

**策略**：先做快速检查，快速淘汰不符合条件的股票

**优化逻辑**：
```python
# 1. 先检查估值（3次API调用，较快）
valuation_pass = check_valuation_pass(...)
if not valuation_pass:
    return None  # 快速淘汰

# 2. 再检查基本面（5次API调用，较慢）
fundamentals_pass = check_fundamentals_pass(...)
```

**加速效果**：
- 如果大部分股票估值不达标，可以提前淘汰
- 节省约 **30-50%** 的时间

**当前实现**：已按此顺序（先估值，后基本面）

---

### 方案5：分批筛选（推荐）

**策略**：不要一次性筛选全部5274只，分批筛选

**实现方法**：
```python
# 方案A：按行业筛选
# 先筛选消费行业（约500只）
# 再筛选科技行业（约800只）
# ...

# 方案B：按市值筛选
# 先筛选大盘股（市值>1000亿，约200只）
# 再筛选中盘股（市值100-1000亿，约1000只）
# ...
```

**加速效果**：
- 单次筛选时间从2小时降到 **10-30分钟**
- 可以随时查看部分结果

---

### 方案6：降低筛选标准（快速筛选）

**策略**：先用宽松标准快速筛选，再对结果精细筛选

**步骤**：
1. 第一轮：宽松标准（PR≤1.5, ROE≥5%），快速筛选出200-300只
2. 第二轮：严格标准（PR≤1.0, ROE≥10%），精细筛选

**加速效果**：
- 第一轮：约 **20-30分钟**（大部分股票快速淘汰）
- 第二轮：约 **5-10分钟**（只筛选200-300只）

---

## 📊 加速方案对比

| 方案 | 难度 | 加速效果 | 推荐度 |
|------|------|---------|--------|
| 提高并发数 | ⭐ 简单 | 2-2.5倍 | ⭐⭐⭐⭐⭐ |
| 减少API延迟 | ⭐⭐ 中等 | 2.5-5倍 | ⭐⭐⭐⭐ |
| 利用缓存 | ⭐ 简单 | 首次2小时，后续10-20分钟 | ⭐⭐⭐⭐⭐ |
| 优化筛选顺序 | ⭐⭐⭐ 复杂 | 30-50% | ⭐⭐⭐ |
| 分批筛选 | ⭐⭐ 中等 | 单次10-30分钟 | ⭐⭐⭐⭐⭐ |
| 降低筛选标准 | ⭐ 简单 | 20-30分钟 | ⭐⭐⭐⭐ |

---

## 🎯 推荐加速策略（组合使用）

### 策略1：快速筛选（推荐新手）

```
1. 提高并发数：max_workers = 10
2. 减少API延迟：api_delay = 0.1（如果积分足够）
3. 分批筛选：先筛选大盘股（约200只）
```

**预期时间**：**15-20分钟**

### 策略2：完整筛选（推荐有经验用户）

```
1. 提高并发数：max_workers = 8-10
2. 减少API延迟：api_delay = 0.1-0.2
3. 利用缓存：首次筛选建立缓存，后续快速
4. 分批筛选：按行业或市值分批
```

**预期时间**：
- 首次：**45分钟-1小时**
- 后续：**10-20分钟**（使用缓存）

### 策略3：极致加速（需要高积分）

```
1. 提高并发数：max_workers = 20-30（高级用户）
2. 减少API延迟：api_delay = 0（高级用户）
3. 利用缓存：充分使用缓存
4. 优化筛选顺序：先估值后基本面
```

**预期时间**：**20-30分钟**

---

## ⚙️ 如何调整参数

### 在Streamlit界面调整

1. 打开"🌐 全网智能筛选"页面
2. 在左侧"⚙️ 配置面板"中调整：
   - **线程数**：从4提升到8-10
   - **API间隔**：从0.5秒降到0.1-0.2秒

### 代码中调整（如果需要）

```python
# screening.py 第364-365行
max_workers: int = 10,  # 从4改为10
api_delay: float = 0.1,  # 从0.5改为0.1
```

---

## 📈 性能监控建议

### 监控指标

1. **处理速度**：股票/分钟
2. **通过率**：通过数/总数
3. **API调用次数**：避免超限
4. **缓存命中率**：提升性能

### 预期性能

| 配置 | 处理速度 | 总耗时（5274只） |
|------|---------|----------------|
| 4线程，0.5秒延迟 | 约2只/分钟 | 约2小时 |
| 10线程，0.1秒延迟 | 约8-10只/分钟 | 约45分钟-1小时 |
| 20线程，0秒延迟 | 约15-20只/分钟 | 约20-30分钟 |

---

## ⚠️ 注意事项

1. **API频率限制**：
   - 根据你的积分等级调整并发数和延迟
   - 避免触发"超过频率限制"错误

2. **缓存使用**：
   - 首次筛选会建立缓存，耗时较长
   - 后续筛选会大幅加速

3. **筛选标准**：
   - 标准越严格，通过率越低
   - 可以先用宽松标准快速筛选，再精细筛选

4. **分批筛选**：
   - 建议不要一次性筛选全部5274只
   - 可以按行业、市值等分批筛选

---

## 🎯 总结

**当前状态**：
- 5274只股票，预计耗时约2小时
- 使用4线程，0.5秒延迟

**推荐优化**：
1. ✅ **提高并发数到10**（最简单，效果明显）
2. ✅ **减少延迟到0.1秒**（如果积分足够）
3. ✅ **分批筛选**（按行业或市值）
4. ✅ **利用缓存**（首次筛选后，后续快速）

**优化后预期**：
- 单次筛选：**45分钟-1小时**
- 后续筛选：**10-20分钟**（使用缓存）

---

**文档生成时间**：2025-01-XX

