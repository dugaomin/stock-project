# -*- coding: utf-8 -*-
"""
å·¥å…·æ¨¡å— - æ•°æ®è·å–å’Œå¤„ç†æ ¸å¿ƒ

åŠŸèƒ½æ¦‚è¿°ï¼š
    å°è£…Tushare Pro APIï¼Œæä¾›è´¢åŠ¡æ•°æ®è·å–ã€è¿é€šæ€§æ£€æµ‹ã€
    æ•°æ®å¤„ç†ç­‰åŠŸèƒ½ï¼Œæ”¯æŒæ™ºèƒ½ç¼“å­˜å’ŒAPIé¢‘ç‡æ§åˆ¶ã€‚

ä¸»è¦æ¨¡å—ï¼š
    1. Tokenç®¡ç† - å®‰å…¨çš„Tokenè·å–å’Œå®¢æˆ·ç«¯åˆå§‹åŒ–
    2. è¿é€šæ€§æ£€æµ‹ - DNSã€HTTPã€Tushare APIä¸‰é‡æ£€æµ‹
    3. æ•°æ®è·å– - å…¬å¸ä¿¡æ¯ã€å®¡è®¡ã€è´¢åŠ¡ä¸‰è¡¨ã€ä¼°å€¼æ•°æ®
    4. ç»¼åˆåˆ†æ - å¤šæ•°æ®æºæ•´åˆï¼Œè®¡ç®—æ ¸å¿ƒæŒ‡æ ‡
    5. æ•°æ®è¿‡æ»¤ - ç­›é€‰å¹´æŠ¥ï¼Œæ’é™¤å­£æŠ¥

APIé¢‘ç‡æ§åˆ¶ï¼š
    æ”¯æŒæ ¹æ®ç”¨æˆ·ç§¯åˆ†ç­‰çº§è®¾ç½®å»¶è¿Ÿï¼Œé¿å…è§¦å‘Tushareé™åˆ¶

ç¼“å­˜æ”¯æŒï¼š
    ä½¿ç”¨cache_manageræ¨¡å—å®ç°24å°æ—¶æŒä¹…åŒ–ç¼“å­˜

ä½œè€…ï¼šgaomindu
"""

from __future__ import annotations  # å…¼å®¹æœªæ¥æ³¨è§£è¯­æ³•

import os  # è¯»å–ç¯å¢ƒå˜é‡
import socket  # DNS æµ‹è¯•
import time  # æ·»åŠ å»¶è¿Ÿæ§åˆ¶
from dataclasses import dataclass  # ç»“æ„åŒ–å®¡è®¡ä¿¡æ¯
from functools import lru_cache  # ç¼“å­˜å®¢æˆ·ç«¯å®ä¾‹
from typing import Any, Dict, List, Optional, Tuple  # ç±»å‹æç¤º

import pandas as pd  # DataFrame å¤„ç†
import requests  # HTTP æµ‹è¯•
import tushare as ts  # Tushare SDK

from settings import DEFAULT_TOKEN  # é»˜è®¤ token
from cache_manager import data_cache  # æ•°æ®ç¼“å­˜

API_HOST = "api.waditu.com"  # å®˜æ–¹æ¥å£åŸŸå


def get_token() -> str:
    """
    è·å–Tushare Pro Token
    
    ä¼˜å…ˆçº§ï¼š
        1. ç¯å¢ƒå˜é‡ TUSHARE_TOKEN
        2. settings.py ä¸­çš„ DEFAULT_TOKEN
    
    Returns:
        Tokenå­—ç¬¦ä¸²
    
    å®‰å…¨æ€§ï¼š
        Tokenä¸åº”ç¡¬ç¼–ç åœ¨ä»£ç ä¸­ï¼Œåº”ä»é…ç½®æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡è¯»å–
    """
    return os.environ.get("TUSHARE_TOKEN", DEFAULT_TOKEN)


@lru_cache(maxsize=1)
def get_pro_client(token: Optional[str] = None):
    """
    è·å–Tushare Pro APIå®¢æˆ·ç«¯ï¼ˆå¸¦ç¼“å­˜ï¼‰
    
    ä½¿ç”¨lru_cacheè£…é¥°å™¨ï¼Œç¡®ä¿å®¢æˆ·ç«¯åªåˆå§‹åŒ–ä¸€æ¬¡ï¼Œé¿å…é‡å¤è¿æ¥ã€‚
    
    Args:
        token: Tushare Tokenï¼Œä¸ä¼ åˆ™ä½¿ç”¨get_token()è·å–
        
    Returns:
        Tushare Pro APIå®¢æˆ·ç«¯å®ä¾‹
    """
    return ts.pro_api(token or get_token())


def get_user_points_info(token: Optional[str] = None) -> Optional[Dict]:
    """
    è·å–ç”¨æˆ·ç§¯åˆ†ä¿¡æ¯ï¼ˆåŒ…æ‹¬åˆ°æœŸç§¯åˆ†ï¼‰
    
    Args:
        token: Tushare Tokenï¼Œä¸ä¼ åˆ™ä½¿ç”¨get_token()è·å–
        
    Returns:
        åŒ…å«ç§¯åˆ†ä¿¡æ¯çš„å­—å…¸ï¼Œå¦‚æœæŸ¥è¯¢å¤±è´¥è¿”å›None
    """
    try:
        pro = get_pro_client(token)
        df = pro.user(token=get_token())
        
        if df.empty:
            return None
        
        # è®¡ç®—æ€»ç§¯åˆ†å’Œæœ€è¿‘åˆ°æœŸæ—¶é—´
        total_points = df['åˆ°æœŸç§¯åˆ†'].sum()
        
        # æ‰¾åˆ°æœ€è¿‘çš„åˆ°æœŸæ—¶é—´
        df['åˆ°æœŸæ—¶é—´'] = pd.to_datetime(df['åˆ°æœŸæ—¶é—´'])
        nearest_expiry = df['åˆ°æœŸæ—¶é—´'].min()
        nearest_expiry_points = df[df['åˆ°æœŸæ—¶é—´'] == nearest_expiry]['åˆ°æœŸç§¯åˆ†'].sum()
        
        return {
            'total_points': float(total_points),
            'nearest_expiry_date': nearest_expiry.strftime('%Y-%m-%d') if pd.notna(nearest_expiry) else None,
            'nearest_expiry_points': float(nearest_expiry_points) if pd.notna(nearest_expiry) else 0,
            'expiry_records': df.to_dict('records')
        }
    except Exception as e:
        print(f"æŸ¥è¯¢ç§¯åˆ†ä¿¡æ¯å¤±è´¥: {e}")
        return None


def run_connectivity_tests(verbose: bool = True) -> Tuple[bool, List[Dict[str, str]]]:
    """
    ç½‘ç»œè¿é€šæ€§ä¸‰é‡æ£€æµ‹
    
    æ£€æµ‹é¡¹ï¼š
        1. DNSè§£æ - æ£€æŸ¥api.waditu.comèƒ½å¦è§£æ
        2. HTTPè¿æ¥ - æ£€æŸ¥HTTPè¯·æ±‚æ˜¯å¦æ­£å¸¸
        3. Tushare API - æ£€æŸ¥APIæ¥å£æ˜¯å¦å¯ç”¨
    
    Args:
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—
        
    Returns:
        (æ˜¯å¦å…¨éƒ¨é€šè¿‡, æ—¥å¿—åˆ—è¡¨)
        
    ç”¨é€”ï¼š
        åœ¨æŸ¥è¯¢æ•°æ®å‰é¢„æ£€æŸ¥ç½‘ç»œç¯å¢ƒï¼Œæå‰å‘ç°é—®é¢˜
    """
    checks = [
        ("DNS è¿é€šæ€§", _dns_check),
        ("HTTP æµ‹è¯•", _http_check),
        ("Tushare API", _tushare_check),
    ]
    success = True
    logs: List[Dict[str, str]] = []
    for title, fn in checks:
        ok, message = fn()
        status = "PASS" if ok else "FAIL"
        log_entry = {"status": status, "title": title, "message": message}
        logs.append(log_entry)
        if verbose:
            print(f"[{status}] {title}ï¼š{message}")
        success = success and ok
    return success, logs


def _dns_check() -> Tuple[bool, str]:
    """æ£€æŸ¥åŸŸåèƒ½å¦è§£æã€‚"""
    try:
        ip_addr = socket.gethostbyname(API_HOST)
        return True, f"{API_HOST} -> {ip_addr}"
    except socket.gaierror as exc:
        return False, f"DNS è§£æå¤±è´¥ï¼š{exc}"


def _http_check() -> Tuple[bool, str]:
    """å‘èµ· HTTP è¯·æ±‚éªŒè¯é“¾è·¯ã€‚"""
    try:
        resp = requests.get(f"http://{API_HOST}", timeout=5)
        return True, f"HTTP çŠ¶æ€ {resp.status_code}"
    except requests.RequestException as exc:
        return False, f"HTTP è¯·æ±‚å¤±è´¥ï¼š{exc}"


def _tushare_check() -> Tuple[bool, str]:
    """è°ƒç”¨æœ€å°æ¥å£éªŒè¯ token/ç½‘ç»œæ˜¯å¦æ­£å¸¸ã€‚"""
    try:
        pro = get_pro_client()
        df = pro.trade_cal(limit=1)
        return True, f"trade_cal è¿”å› {len(df)} æ¡è®°å½•"
    except Exception as exc:  # noqa: BLE001
        return False, f"Tushare è°ƒç”¨å¤±è´¥ï¼š{exc}"


@dataclass
class AuditRecord:
    """æ¯ä¸ªæŠ¥å‘ŠæœŸçš„å®¡è®¡æ„è§ã€‚"""

    ann_date: str
    end_date: str
    audit_result: str
    audit_agency: str
    audit_sign: str

    @property
    def is_standard(self) -> bool:
        """æ˜¯å¦ä¸ºæ ‡å‡†æ— ä¿ç•™æ„è§ã€‚"""
        return "æ ‡å‡†æ— ä¿ç•™æ„è§" in (self.audit_result or "")


def fetch_valuation_data(
    ts_code: str,
    trade_date: str,
    target_type: str = "ä¸ªè‚¡",
) -> Optional[Dict[str, Any]]:
    """
    è·å–å¸‚èµšç‡è®¡ç®—æ‰€éœ€çš„ä¼°å€¼æ•°æ®
    
    Args:
        ts_code: è‚¡ç¥¨ä»£ç 
        trade_date: äº¤æ˜“æ—¥æœŸï¼Œæ ¼å¼YYYYMMDD
        target_type: æ ‡çš„ç±»å‹ï¼ˆä¸ªè‚¡/å®½åŸºæŒ‡æ•°ï¼‰
        
    Returns:
        åŒ…å«pe_ttm, roe_waa, eps, dividendç­‰å­—æ®µçš„å­—å…¸
    """
    try:
        pro = get_pro_client()
        
        # 1. è·å–æ¯æ—¥æŒ‡æ ‡ï¼ˆpe_ttm, closeç­‰ï¼‰
        pe_ttm = None
        close_price = None
        data_source = "daily_basic"

        if target_type == "å®½åŸºæŒ‡æ•°":
            index_df = pro.index_dailybasic(
                ts_code=ts_code,
                trade_date=trade_date,
                fields="ts_code,trade_date,close,pe_ttm"
            )
            if index_df.empty and trade_date:
                # è‹¥æŒ‡å®šæ—¥æœŸæ— æ•°æ®ï¼Œå°è¯•è·å–æœ€è¿‘ä¸€æœŸ
                index_df = pro.index_dailybasic(
                    ts_code=ts_code,
                    end_date=trade_date,
                    fields="ts_code,trade_date,close,pe_ttm",
                    limit=1
                )
            if not index_df.empty:
                latest = index_df.sort_values("trade_date", ascending=False).iloc[0]
                pe_ttm = latest.get("pe_ttm")
                close_price = latest.get("close")
                data_source = "index_dailybasic"
        else:
            daily_df = pro.daily_basic(
                ts_code=ts_code,
                trade_date=trade_date,
                fields="ts_code,trade_date,close,pe_ttm"
            )
            if daily_df.empty and trade_date:
                daily_df = pro.daily_basic(
                    ts_code=ts_code,
                    end_date=trade_date,
                    fields="ts_code,trade_date,close,pe_ttm",
                    limit=1
                )
            if not daily_df.empty:
                latest = daily_df.sort_values("trade_date", ascending=False).iloc[0]
                pe_ttm = latest.get("pe_ttm")
                close_price = latest.get("close")
        
        if pe_ttm is None and close_price is None:
            print(f"âš ï¸ æœªè·å–åˆ°{ts_code}åœ¨{trade_date}çš„ä¼°å€¼åŸºç¡€æ•°æ®")
        
        # 2. è·å–è´¢åŠ¡æŒ‡æ ‡ï¼ˆroe_waa, epsï¼‰- è·å–æœ€æ–°çš„è´¢åŠ¡æ•°æ®
        roe_waa = None
        eps = None
        
        if target_type == "ä¸ªè‚¡":
            # å…ˆå°è¯•ä½¿ç”¨æˆªæ­¢åˆ°trade_dateçš„æœ€è¿‘è´¢æŠ¥
            fina_df = pro.fina_indicator(
                ts_code=ts_code,
                end_date=trade_date,
                fields="ts_code,end_date,roe_waa,eps",
                limit=1
            )
            if fina_df.empty:
                # å¦‚æœä»ä¸ºç©ºï¼Œé€€è€Œæ±‚å…¶æ¬¡å–æœ€è¿‘æŠ«éœ²çš„è´¢æŠ¥
                fina_df = pro.fina_indicator(
                    ts_code=ts_code,
                    fields="ts_code,end_date,roe_waa,eps",
                    limit=1
                )
            if not fina_df.empty:
                fina_row = fina_df.sort_values("end_date", ascending=False).iloc[0]
                roe_waa = fina_row.get("roe_waa")
                eps = fina_row.get("eps")
            else:
                print(f"âš ï¸ {ts_code} æœªè·å–åˆ°è´¢åŠ¡æŒ‡æ ‡æ•°æ®ï¼ˆroe_waa / epsï¼‰")
        
        # 3. è·å–åˆ†çº¢æ•°æ® - è·å–æœ€è¿‘ä¸€æ¬¡åˆ†çº¢
        dividend_per_share = None
        if target_type == "ä¸ªè‚¡":
            div_df = pro.dividend(
                ts_code=ts_code,
                end_date=trade_date,
                fields="ts_code,div_proc,cash_div,ex_date,record_date,ann_date,imp_ann_date",
                limit=30
            )
            if div_df.empty:
                div_df = pro.dividend(
                    ts_code=ts_code,
                    fields="ts_code,div_proc,cash_div,ex_date,record_date,ann_date,imp_ann_date",
                    limit=30
                )
            if not div_df.empty:
                div_df = div_df.sort_values("ex_date", ascending=False)
                executed = div_df[
                    (div_df["div_proc"].fillna("") == "å®æ–½") & (div_df["cash_div"].notna()) & (div_df["cash_div"] > 0)
                ]
                source_df = executed if not executed.empty else div_df
                latest_div = source_df.iloc[0]
                dividend_per_share = latest_div.get("cash_div")
        
        return {
            'ts_code': ts_code,
            'trade_date': trade_date,
            'close': close_price,
            'pe_ttm': pe_ttm,
            'roe_waa': roe_waa,
            'eps': eps,
            'dividend_per_share': dividend_per_share,
            'data_source': data_source,
        }
        
    except Exception as e:
        print(f"è·å–ä¼°å€¼æ•°æ®å¤±è´¥: {e}")
        return None


def fetch_company_info(ts_code: str) -> Optional[Dict[str, Any]]:
    """è·å–ä¸Šå¸‚å…¬å¸åŸºæœ¬ä¿¡æ¯"""
    try:
        pro = get_pro_client()
        df = pro.stock_company(
            ts_code=ts_code,
            fields='ts_code,com_name,chairman,manager,secretary,reg_capital,setup_date,province,city,introduction,website,email,employees,main_business,business_scope'
        )
        if df.empty:
            return None
        
        row = df.iloc[0]
        return {
            'ts_code': row.get('ts_code', ''),
            'com_name': row.get('com_name', ''),
            'chairman': row.get('chairman', ''),
            'manager': row.get('manager', ''),
            'secretary': row.get('secretary', ''),
            'reg_capital': row.get('reg_capital', 0),
            'setup_date': row.get('setup_date', ''),
            'province': row.get('province', ''),
            'city': row.get('city', ''),
            'introduction': row.get('introduction', ''),
            'website': row.get('website', ''),
            'email': row.get('email', ''),
            'employees': row.get('employees', 0),
            'main_business': row.get('main_business', ''),
            'business_scope': row.get('business_scope', ''),
        }
    except Exception as e:
        print(f"è·å–å…¬å¸ä¿¡æ¯å¤±è´¥: {e}")
        return None


def fetch_audit_records(
    ts_code: str,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    limit: int = 20,
) -> List[AuditRecord]:
    """è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„å®¡è®¡æ„è§åˆ—è¡¨ã€‚"""
    pro = get_pro_client()
    fields = "ann_date,end_date,audit_result,audit_agency,audit_sign"
    # å¦‚æœè®¾ç½®äº†æ—¥æœŸèŒƒå›´ï¼Œä½¿ç”¨æ›´å¤§çš„limitç¡®ä¿è·å–æ‰€æœ‰æ•°æ®
    api_limit = max(limit * 3, 200) if (start_date and end_date) else limit
    if start_date and end_date:
        print(f"ğŸ“Š è°ƒç”¨fina_audit APIï¼Œlimit={api_limit}ï¼Œæ—¥æœŸèŒƒå›´ï¼š{start_date} - {end_date}")
    params: Dict[str, Any] = {
        "ts_code": ts_code,
        "start_date": start_date,
        "end_date": end_date,
        "limit": api_limit,
        "fields": fields,
    }
    params = {k: v for k, v in params.items() if v is not None}
    df = pro.fina_audit(**params)
    if df.empty:
        raise ValueError("æœªè·å–åˆ°å®¡è®¡æ„è§ï¼Œè¯·ç¡®è®¤æƒé™æˆ–æŠ«éœ²æƒ…å†µã€‚")
    # ä¸å†ä½¿ç”¨headé™åˆ¶ï¼Œå› ä¸ºå·²ç»é€šè¿‡start_dateå’Œend_dateæ­£ç¡®è¿‡æ»¤äº†
    df = df.sort_values("end_date", ascending=False)
    records = [
        AuditRecord(
            ann_date=row["ann_date"],
            end_date=row["end_date"],
            audit_result=row["audit_result"],
            audit_agency=row["audit_agency"],
            audit_sign=row["audit_sign"],
        )
        for _, row in df.iterrows()
    ]
    return records


def fetch_balancesheet(
    ts_code: str,
    start_date: Optional[str],
    end_date: Optional[str],
    max_records: int,
) -> pd.DataFrame:
    """è·å–èµ„äº§è´Ÿå€ºè¡¨æ•°æ®ã€‚"""
    pro = get_pro_client()
    fields = "ts_code,ann_date,end_date,total_assets,total_liab"
    # å¦‚æœè®¾ç½®äº†æ—¥æœŸèŒƒå›´ï¼Œä½¿ç”¨æ›´å¤§çš„limitç¡®ä¿è·å–æ‰€æœ‰æ•°æ®
    # å¯¹äº1995-2024è¿™æ ·çš„èŒƒå›´ï¼Œéœ€è¦è¶³å¤Ÿå¤§çš„limit
    api_limit = max(max_records * 3, 200)  # è‡³å°‘200æ¡ï¼Œæˆ–è€…max_recordsçš„3å€
    print(f"ğŸ“Š è°ƒç”¨balancesheet APIï¼Œlimit={api_limit}ï¼Œæ—¥æœŸèŒƒå›´ï¼š{start_date} - {end_date}")
    df = pro.balancesheet(
        ts_code=ts_code,
        start_date=start_date,
        end_date=end_date,
        fields=fields,
        limit=api_limit,
    )
    return _filter_annual_records(
        df,
        start_date,
        end_date,
        ["total_assets", "total_liab"],
        max_records,
    )


def fetch_income(
    ts_code: str,
    start_date: Optional[str],
    end_date: Optional[str],
    max_records: int,
) -> pd.DataFrame:
    """è·å–åˆ©æ¶¦è¡¨æ•°æ®ã€‚"""
    pro = get_pro_client()
    fields = "ts_code,ann_date,end_date,revenue,oper_cost,n_income"
    # å¦‚æœè®¾ç½®äº†æ—¥æœŸèŒƒå›´ï¼Œä½¿ç”¨æ›´å¤§çš„limitç¡®ä¿è·å–æ‰€æœ‰æ•°æ®
    api_limit = max(max_records * 3, 200)  # è‡³å°‘200æ¡ï¼Œæˆ–è€…max_recordsçš„3å€
    print(f"ğŸ“Š è°ƒç”¨income APIï¼Œlimit={api_limit}ï¼Œæ—¥æœŸèŒƒå›´ï¼š{start_date} - {end_date}")
    df = pro.income(
        ts_code=ts_code,
        start_date=start_date,
        end_date=end_date,
        fields=fields,
        limit=api_limit,
    )
    return _filter_annual_records(
        df,
        start_date,
        end_date,
        ["revenue", "oper_cost", "n_income"],
        max_records,
    )


def fetch_cashflow(
    ts_code: str,
    start_date: Optional[str],
    end_date: Optional[str],
    max_records: int,
) -> pd.DataFrame:
    """è·å–ç°é‡‘æµé‡è¡¨æ•°æ®ã€‚"""
    pro = get_pro_client()
    fields = "ts_code,ann_date,end_date,n_cashflow_act"
    # å¦‚æœè®¾ç½®äº†æ—¥æœŸèŒƒå›´ï¼Œä½¿ç”¨æ›´å¤§çš„limitç¡®ä¿è·å–æ‰€æœ‰æ•°æ®
    api_limit = max(max_records * 3, 200)  # è‡³å°‘200æ¡ï¼Œæˆ–è€…max_recordsçš„3å€
    print(f"ğŸ“Š è°ƒç”¨cashflow APIï¼Œlimit={api_limit}ï¼Œæ—¥æœŸèŒƒå›´ï¼š{start_date} - {end_date}")
    df = pro.cashflow(
        ts_code=ts_code,
        start_date=start_date,
        end_date=end_date,
        fields=fields,
        limit=api_limit,
    )
    return _filter_annual_records(
        df,
        start_date,
        end_date,
        ["n_cashflow_act"],
        max_records,
    )


def analyze_fundamentals(
    ts_code: str,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    years: int = 5,
    use_cache: bool = True,
    api_delay: int = 31,
    progress_callback=None,
) -> Dict[str, Any]:
    """
    æ‰§è¡Œç»¼åˆåˆ†æï¼Œè®¡ç®—èµ„äº§è´Ÿå€ºç‡ã€æ¯›åˆ©ç‡ã€ç»è¥ç°é‡‘æµç­‰æŒ‡æ ‡ã€‚
    
    Args:
        ts_code: è‚¡ç¥¨ä»£ç 
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        years: å¹´æ•°
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜

    Returns:
        dict: åŒ…å«å®¡è®¡ä¿¡æ¯ã€æŒ‡æ ‡ DataFrameã€ç°é‡‘æµç»Ÿè®¡ç­‰æ•°æ®ã€‚
    """
    # ç”Ÿæˆç¼“å­˜é”®ï¼ˆåŒ…å«å®Œæ•´çš„æ—¥æœŸèŒƒå›´ï¼Œç¡®ä¿å¹´ä»½å˜åŒ–æ—¶ç¼“å­˜é”®ä¹Ÿå˜åŒ–ï¼‰
    cache_key = f"{ts_code}_{start_date}_{end_date}_{years}"
    print(f"ğŸ”‘ ç¼“å­˜é”®ï¼š{cache_key}")
    
    # å…ˆæ£€æŸ¥ç¼“å­˜
    if use_cache:
        cached_data = data_cache.get(cache_key)
        if cached_data is not None:
            try:
                # å°†cached_dataä¸­çš„DataFrameè½¬å›pandas DataFrame
                if 'metrics_dict' in cached_data and cached_data['metrics_dict']:
                    metrics_df = pd.DataFrame(cached_data['metrics_dict'])
                    
                    # å°†audit_records dictè½¬å›AuditRecordå¯¹è±¡
                    audit_list = []
                    if 'audit_records' in cached_data and isinstance(cached_data['audit_records'], list):
                        audit_list = [
                            AuditRecord(**r) if isinstance(r, dict) else r
                            for r in cached_data['audit_records']
                        ]
                    
                    # é‡æ–°æ„å»ºå®Œæ•´çš„resultå¯¹è±¡
                    result = {
                        'company_info': cached_data.get('company_info'),
                        'metrics': metrics_df,
                        'audit_records': audit_list,
                        'cashflow_positive_years': cached_data.get('cashflow_positive_years', 0),
                        'cashflow_cover_profit': cached_data.get('cashflow_cover_profit', False)
                    }
                    
                    print(f"âœ… ä»ç¼“å­˜åŠ è½½æ•°æ®ï¼š{len(metrics_df)}å¹´æ•°æ®ï¼ˆæ—¥æœŸèŒƒå›´ï¼š{start_date} - {end_date}ï¼‰")
                    # éªŒè¯ç¼“å­˜æ•°æ®çš„å¹´ä»½èŒƒå›´æ˜¯å¦æ­£ç¡®
                    if not metrics_df.empty:
                        cached_years = sorted([row['end_date'][:4] for _, row in metrics_df.iterrows()])
                        print(f"ğŸ“… ç¼“å­˜æ•°æ®åŒ…å«çš„å¹´ä»½ï¼š{cached_years}")
                    return result
                else:
                    print("âš ï¸ ç¼“å­˜æ•°æ®æ ¼å¼å¼‚å¸¸ï¼ˆæ— metrics_dictï¼‰ï¼Œåˆ é™¤å¹¶é‡æ–°è·å–")
                    data_cache.delete(cache_key)
            except Exception as e:
                print(f"âš ï¸ ç¼“å­˜æ•°æ®è§£æå¤±è´¥ï¼Œåˆ é™¤å¹¶é‡æ–°è·å–: {e}")
                data_cache.delete(cache_key)
    
    # ç¼“å­˜æœªå‘½ä¸­æˆ–å¼‚å¸¸ï¼Œè°ƒç”¨APIè·å–æ•°æ®
    # å¦‚æœæŒ‡å®šäº†æ—¥æœŸèŒƒå›´ï¼Œæ ¹æ®æ—¥æœŸèŒƒå›´è®¡ç®—éœ€è¦çš„è®°å½•æ•°
    if start_date and end_date:
        # è®¡ç®—å¹´ä»½è·¨åº¦ï¼ˆä¾‹å¦‚ï¼š19950101 åˆ° 20501231 = 56å¹´ï¼‰
        start_year = int(start_date[:4])
        end_year = int(end_date[:4])
        max_records = end_year - start_year + 1
        # ä½†ä¹Ÿè¦è®¾ç½®ä¸€ä¸ªåˆç†çš„ä¸Šé™ï¼Œé¿å…è¯·æ±‚è¿‡å¤šæ•°æ®ï¼ˆæ¯”å¦‚æœ€å¤š100å¹´ï¼‰
        max_records = min(max_records, 100)
        print(f"ğŸ“Š æ ¹æ®æ—¥æœŸèŒƒå›´è®¡ç®—max_recordsï¼š{start_year}-{end_year} = {max_records}å¹´")
    elif start_date or end_date:
        # åªæŒ‡å®šäº†å¼€å§‹æˆ–ç»“æŸæ—¥æœŸï¼Œä½¿ç”¨é»˜è®¤å€¼
        max_records = 20
    else:
        # æ²¡æœ‰æŒ‡å®šæ—¥æœŸèŒƒå›´ï¼Œä½¿ç”¨yearså‚æ•°
        max_records = years
    
    # ä¸ºäº†é¿å…è§¦å‘é¢‘ç‡é™åˆ¶ï¼Œåœ¨æ¯æ¬¡APIè°ƒç”¨ä¹‹é—´æ·»åŠ å»¶è¿Ÿ
    # å…è´¹ç”¨æˆ·(0-119åˆ†)ï¼šæ¯åˆ†é’Ÿ2æ¬¡ â†’ éœ€è¦é—´éš”31ç§’
    # æ³¨å†Œç”¨æˆ·(120-599åˆ†)ï¼šæ¯åˆ†é’Ÿ5æ¬¡ â†’ é—´éš”13ç§’
    # ä¸­çº§ç”¨æˆ·(600-4999åˆ†)ï¼šæ¯åˆ†é’Ÿ20æ¬¡ â†’ é—´éš”4ç§’
    # é«˜çº§ç”¨æˆ·(5000+åˆ†)ï¼šæ¯åˆ†é’Ÿ200æ¬¡ â†’ æ— éœ€å»¶è¿Ÿ
    
    # ç¬¬1æ¬¡è°ƒç”¨ï¼šå…¬å¸åŸºæœ¬ä¿¡æ¯
    if progress_callback:
        progress_callback("æ­£åœ¨è·å–å…¬å¸åŸºæœ¬ä¿¡æ¯... (1/5)", 0.20)
    company_info = fetch_company_info(ts_code)
    print(f"âœ… å·²è·å–å…¬å¸ä¿¡æ¯")
    
    if api_delay > 0:
        print(f"â° ç­‰å¾…{api_delay}ç§’...")
        time.sleep(api_delay)
    
    # ç¬¬2æ¬¡è°ƒç”¨ï¼šå®¡è®¡æ„è§
    if progress_callback:
        progress_callback("æ­£åœ¨è·å–å®¡è®¡æ„è§... (2/5)", 0.40)
    print(f"ğŸ“… æŸ¥è¯¢æ—¥æœŸèŒƒå›´ï¼šstart_date={start_date}, end_date={end_date}")
    audit_records = fetch_audit_records(ts_code, start_date, end_date, max_records)
    print(f"âœ… å·²è·å–å®¡è®¡æ„è§ï¼Œå…±{len(audit_records)}æ¡è®°å½•")
    
    if api_delay > 0:
        time.sleep(api_delay)
    
    # ç¬¬3æ¬¡è°ƒç”¨ï¼šèµ„äº§è´Ÿå€ºè¡¨
    if progress_callback:
        progress_callback("æ­£åœ¨è·å–èµ„äº§è´Ÿå€ºè¡¨... (3/5)", 0.60)
    balance_df = fetch_balancesheet(ts_code, start_date, end_date, max_records)
    print(f"âœ… å·²è·å–èµ„äº§è´Ÿå€ºè¡¨")
    
    if api_delay > 0:
        time.sleep(api_delay)
    
    # ç¬¬4æ¬¡è°ƒç”¨ï¼šåˆ©æ¶¦è¡¨
    if progress_callback:
        progress_callback("æ­£åœ¨è·å–åˆ©æ¶¦è¡¨... (4/5)", 0.80)
    income_df = fetch_income(ts_code, start_date, end_date, max_records)
    print(f"âœ… å·²è·å–åˆ©æ¶¦è¡¨")
    
    if api_delay > 0:
        time.sleep(api_delay)
    
    # ç¬¬5æ¬¡è°ƒç”¨ï¼šç°é‡‘æµé‡è¡¨
    if progress_callback:
        progress_callback("æ­£åœ¨è·å–ç°é‡‘æµé‡è¡¨... (5/5)", 1.0)
    cashflow_df = fetch_cashflow(ts_code, start_date, end_date, max_records)
    print("âœ… å·²è·å–ç°é‡‘æµé‡è¡¨ï¼Œæ•°æ®æ”¶é›†å®Œæˆï¼")
    print(f"ğŸ“Š è·å–åˆ°çš„åŸå§‹æ•°æ®ç»Ÿè®¡ï¼š")
    if not balance_df.empty:
        balance_years = sorted([row['end_date'][:4] for _, row in balance_df.iterrows()])
        print(f"  - èµ„äº§è´Ÿå€ºè¡¨ï¼š{len(balance_df)}æ¡è®°å½•ï¼Œå¹´ä»½èŒƒå›´ï¼š{balance_years[0] if balance_years else 'N/A'} - {balance_years[-1] if balance_years else 'N/A'}")
    if not income_df.empty:
        income_years = sorted([row['end_date'][:4] for _, row in income_df.iterrows()])
        print(f"  - åˆ©æ¶¦è¡¨ï¼š{len(income_df)}æ¡è®°å½•ï¼Œå¹´ä»½èŒƒå›´ï¼š{income_years[0] if income_years else 'N/A'} - {income_years[-1] if income_years else 'N/A'}")
    if not cashflow_df.empty:
        cashflow_years = sorted([row['end_date'][:4] for _, row in cashflow_df.iterrows()])
        print(f"  - ç°é‡‘æµé‡è¡¨ï¼š{len(cashflow_df)}æ¡è®°å½•ï¼Œå¹´ä»½èŒƒå›´ï¼š{cashflow_years[0] if cashflow_years else 'N/A'} - {cashflow_years[-1] if cashflow_years else 'N/A'}")

    merged = (
        balance_df[["end_date", "total_assets", "total_liab"]]
        .merge(
            income_df[["end_date", "revenue", "oper_cost", "n_income"]],
            on="end_date",
            how="inner",
        )
        .merge(
            cashflow_df[["end_date", "n_cashflow_act"]],
            on="end_date",
            how="left",
        )
        .sort_values("end_date", ascending=False)
        .reset_index(drop=True)
    )
    
    # æ‰“å°åˆå¹¶åçš„æ•°æ®å¹´ä»½èŒƒå›´
    if not merged.empty:
        merged_years = sorted([row['end_date'][:4] for _, row in merged.iterrows()])
        print(f"ğŸ“… åˆå¹¶åçš„æ•°æ®å¹´ä»½ï¼š{merged_years}ï¼ˆå…±{len(merged)}å¹´ï¼‰")
        print(f"ğŸ“… æ•°æ®å¹´ä»½èŒƒå›´ï¼š{merged_years[0]} - {merged_years[-1]}")
        print(f"ğŸ“… æœŸæœ›çš„å¹´ä»½èŒƒå›´ï¼š{start_date[:4]} - {end_date[:4]}")
        if merged_years[0] != start_date[:4] or merged_years[-1] != end_date[:4]:
            print(f"âš ï¸ è­¦å‘Šï¼šåˆå¹¶åçš„æ•°æ®å¹´ä»½èŒƒå›´ä¸æœŸæœ›èŒƒå›´ä¸ä¸€è‡´ï¼")
    else:
        print(f"âš ï¸ è­¦å‘Šï¼šåˆå¹¶åçš„æ•°æ®ä¸ºç©ºï¼")

    merged["debt_ratio"] = merged["total_liab"] / merged["total_assets"]
    
    # è®¡ç®—æ¯›åˆ©ç‡ï¼šéœ€è¦æ£€æŸ¥revenueæ˜¯å¦ä¸º0æˆ–NaN
    # å¦‚æœrevenueä¸º0æˆ–NaNï¼Œåˆ™æ¯›åˆ©ç‡ä¸ºNaNï¼ˆè¡¨ç¤ºæ•°æ®ç¼ºå¤±ï¼‰
    def calc_gross_margin(row):
        revenue = row.get('revenue', 0)
        oper_cost = row.get('oper_cost', 0)
        if pd.isna(revenue) or revenue == 0:
            return pd.NA  # è¿”å›NaNè¡¨ç¤ºæ•°æ®ç¼ºå¤±
        if pd.isna(oper_cost):
            return pd.NA  # å¦‚æœæˆæœ¬ç¼ºå¤±ï¼Œä¹Ÿæ— æ³•è®¡ç®—æ¯›åˆ©ç‡
        return (revenue - oper_cost) / revenue
    
    merged["gross_margin"] = merged.apply(calc_gross_margin, axis=1)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ¯›åˆ©ç‡ç¼ºå¤±çš„æƒ…å†µï¼Œå¹¶æ‰“å°è°ƒè¯•ä¿¡æ¯
    missing_gross_margin = merged["gross_margin"].isna().sum()
    if missing_gross_margin > 0:
        print(f"âš ï¸ è­¦å‘Šï¼šæœ‰ {missing_gross_margin} å¹´çš„æ¯›åˆ©ç‡æ•°æ®ç¼ºå¤±ï¼ˆå¯èƒ½æ˜¯è´¢æŠ¥ä¸­revenueæˆ–oper_costå­—æ®µç¼ºå¤±ï¼‰")
        # æ‰“å°ç¼ºå¤±çš„å¹´ä»½å’ŒåŸå› 
        for idx, row in merged[merged["gross_margin"].isna()].iterrows():
            revenue = row.get('revenue', 0)
            oper_cost = row.get('oper_cost', 0)
            year = row['end_date'][:4] if pd.notna(row.get('end_date')) else 'æœªçŸ¥'
            if pd.isna(revenue) or revenue == 0:
                print(f"  - {year}å¹´ï¼šrevenueç¼ºå¤±æˆ–ä¸º0 (revenue={revenue})")
            elif pd.isna(oper_cost):
                print(f"  - {year}å¹´ï¼šoper_costç¼ºå¤± (oper_cost={oper_cost})")
    
    merged["cashflow_positive"] = merged["n_cashflow_act"] > 0
    merged["cashflow_ge_profit"] = merged["n_cashflow_act"] >= merged["n_income"]

    result = {
        "company_info": company_info,
        "audit_records": audit_records,
        "metrics": merged,
        "cashflow_positive_years": int(merged["cashflow_positive"].sum()),
        "cashflow_cover_profit": bool(merged["cashflow_ge_profit"].all()),
    }
    
    # ä¿å­˜åˆ°ç¼“å­˜
    if use_cache:
        # å‡†å¤‡å¯åºåˆ—åŒ–çš„ç¼“å­˜æ•°æ®
        cache_data = {
            'company_info': company_info,  # å…¬å¸ä¿¡æ¯
            'metrics_dict': merged.to_dict('records'),  # DataFrameè½¬dict
            'cashflow_positive_years': int(merged["cashflow_positive"].sum()),
            'cashflow_cover_profit': bool(merged["cashflow_ge_profit"].all()),
            'audit_records': [
                {
                    'ann_date': r.ann_date,
                    'end_date': r.end_date,
                    'audit_result': r.audit_result,
                    'audit_agency': r.audit_agency,
                    'audit_sign': r.audit_sign,
                }
                for r in audit_records
            ]
        }
        
        saved = data_cache.set(cache_key, cache_data)
        if saved:
            print(f"âœ… æ•°æ®å·²ç¼“å­˜ï¼š{cache_key}")
        else:
            print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥")
    
    return result


def _filter_annual_records(
    df: pd.DataFrame,
    start_date: Optional[str],
    end_date: Optional[str],
    value_columns: List[str],
    max_records: int,
) -> pd.DataFrame:
    """ç­›é€‰å¹´æŠ¥å¹¶è½¬æ¢å­—æ®µç±»å‹ã€‚"""
    if df.empty:
        raise ValueError("æ¥å£è¿”å›ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ ts_code æˆ–æƒé™ã€‚")

    df["end_date"] = df["end_date"].astype(str)
    df = df[df["end_date"].str.endswith("1231")]
    if df.empty:
        raise ValueError("æœªæŸ¥è¯¢åˆ°å¹´æŠ¥æ•°æ®ï¼Œè¯·ç¡®è®¤å…¬å¸æ˜¯å¦æŠ«éœ²å¹´æŠ¥ã€‚")

    if start_date:
        df = df[df["end_date"] >= start_date]
    if end_date:
        df = df[df["end_date"] <= end_date]
    if df.empty:
        raise ValueError("æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰å¹´æŠ¥æ•°æ®ï¼Œè¯·è°ƒæ•´æ—¶é—´åŒºé—´ã€‚")

    df = df.sort_values("end_date", ascending=False)
    df = df.drop_duplicates(subset="end_date", keep="first")
    
    # é‡è¦ä¿®å¤ï¼šå¦‚æœç”¨æˆ·è®¾ç½®äº†æ—¥æœŸèŒƒå›´ï¼ˆå¦‚1995-2050ï¼‰ï¼Œåº”è¯¥è¿”å›è¯¥èŒƒå›´å†…çš„æ‰€æœ‰æ•°æ®
    # è€Œä¸æ˜¯è¢«max_recordsé™åˆ¶ã€‚ä½†å¦‚æœæ•°æ®é‡è¶…è¿‡max_recordsï¼Œè¯´æ˜å¯èƒ½æœ‰é—®é¢˜ï¼Œç»™å‡ºè­¦å‘Š
    if max_records and len(df) > max_records:
        print(f"âš ï¸ è­¦å‘Šï¼šè·å–åˆ°{len(df)}æ¡è®°å½•ï¼Œä½†max_records={max_records}")
        print(f"   å®é™…å¹´ä»½èŒƒå›´ï¼š{df['end_date'].min()} - {df['end_date'].max()}")
        print(f"   å°†è¿”å›æ‰€æœ‰ç¬¦åˆæ—¥æœŸèŒƒå›´çš„æ•°æ®ï¼ˆ{len(df)}æ¡ï¼‰ï¼Œè€Œä¸æ˜¯åªè¿”å›æœ€è¿‘{max_records}æ¡")
    # ä¸å†ä½¿ç”¨headé™åˆ¶ï¼Œå› ä¸ºå·²ç»é€šè¿‡start_dateå’Œend_dateæ­£ç¡®è¿‡æ»¤äº†
    df = df.copy()

    for col in value_columns:
        df[col] = pd.to_numeric(df[col], errors="coerce")

    df = df.dropna(subset=value_columns, how="all")
    return df
