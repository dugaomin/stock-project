# -*- coding: utf-8 -*-
"""
å·¥å…·æ¨¡å— - æ•°æ®è·å–å’Œå¤„ç†æ ¸å¿ƒ

åŠŸèƒ½æ¦‚è¿°ï¼š
    å°è£…Tushare Pro APIï¼Œæä¾›è´¢åŠ¡æ•°æ®è·å–ã€è¿é€šæ€§æ£€æµ‹ã€
    æ•°æ®å¤„ç†ç­‰åŠŸèƒ½ï¼Œæ”¯æŒæ™ºèƒ½ç¼“å­˜å’ŒAPIé¢‘ç‡æ§åˆ¶ã€‚

ä¸»è¦æ¨¡å—ï¼š
    1. Tokenç®¡ç† - å®‰å…¨çš„Tokenè·å–å’Œå®¢æˆ·ç«¯åˆå§‹åŒ–
    2. è¿é€šæ€§æ£€æµ‹ - DNSã€HTTPã€Tushare APIä¸‰é‡æ£€æµ‹
    3. æ•°æ®è·å– - å…¬å¸ä¿¡æ¯ã€å®¡è®¡ã€è´¢åŠ¡ä¸‰è¡¨ã€ä¼°å€¼æ•°æ®
    4. ç»¼åˆåˆ†æ - å¤šæ•°æ®æºæ•´åˆï¼Œè®¡ç®—æ ¸å¿ƒæŒ‡æ ‡
    5. æ•°æ®è¿‡æ»¤ - ç­›é€‰å¹´æŠ¥ï¼Œæ’é™¤å­£æŠ¥

APIé¢‘ç‡æ§åˆ¶ï¼š
    æ”¯æŒæ ¹æ®ç”¨æˆ·ç§¯åˆ†ç­‰çº§è®¾ç½®å»¶è¿Ÿï¼Œé¿å…è§¦å‘Tushareé™åˆ¶

ç¼“å­˜æ”¯æŒï¼š
    ä½¿ç”¨cache_manageræ¨¡å—å®ç°24å°æ—¶æŒä¹…åŒ–ç¼“å­˜

ä½œè€…ï¼šgaomindu
"""

from __future__ import annotations  # å…¼å®¹æœªæ¥æ³¨è§£è¯­æ³•

import os  # è¯»å–ç¯å¢ƒå˜é‡
import socket  # DNS æµ‹è¯•
import time  # æ·»åŠ å»¶è¿Ÿæ§åˆ¶
import json  # JSONå¤„ç†ï¼ˆç”¨äºç¼“å­˜ï¼‰
from datetime import datetime  # æ—¥æœŸæ—¶é—´å¤„ç†
from dataclasses import dataclass  # ç»“æ„åŒ–å®¡è®¡ä¿¡æ¯
from functools import lru_cache  # ç¼“å­˜å®¢æˆ·ç«¯å®ä¾‹
from typing import Any, Dict, List, Optional, Tuple  # ç±»å‹æç¤º

import pandas as pd  # DataFrame å¤„ç†
import requests  # HTTP æµ‹è¯•
import tushare as ts  # Tushare SDK

from settings import DEFAULT_TOKEN  # é»˜è®¤ token
from cache_manager import data_cache  # æ•°æ®ç¼“å­˜

API_HOST = "api.waditu.com"  # å®˜æ–¹æ¥å£åŸŸå


def get_token() -> str:
    """
    è·å–Tushare Pro Token
    
    ä¼˜å…ˆçº§ï¼š
        1. ç¯å¢ƒå˜é‡ TUSHARE_TOKEN
        2. settings.py ä¸­çš„ DEFAULT_TOKEN
    
    è¿”å›:
        Tokenå­—ç¬¦ä¸²
    
    å®‰å…¨æ€§ï¼š
        Tokenä¸åº”ç¡¬ç¼–ç åœ¨ä»£ç ä¸­ï¼Œåº”ä»é…ç½®æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡è¯»å–
    """
    return os.environ.get("TUSHARE_TOKEN", DEFAULT_TOKEN)


@lru_cache(maxsize=1)
def get_pro_client(token: Optional[str] = None):
    """
    è·å–Tushare Pro APIå®¢æˆ·ç«¯ï¼ˆå¸¦ç¼“å­˜ï¼‰
    
    ä½¿ç”¨lru_cacheè£…é¥°å™¨ï¼Œç¡®ä¿å®¢æˆ·ç«¯åªåˆå§‹åŒ–ä¸€æ¬¡ï¼Œé¿å…é‡å¤è¿æ¥ã€‚
    
    å‚æ•°:
        token: Tushare Tokenï¼Œä¸ä¼ åˆ™ä½¿ç”¨get_token()è·å–
        
    è¿”å›:
        Tushare Pro APIå®¢æˆ·ç«¯å®ä¾‹
    """
    return ts.pro_api(token or get_token())


def get_user_points_info(token: Optional[str] = None) -> Optional[Dict]:
    """
    è·å–ç”¨æˆ·ç§¯åˆ†ä¿¡æ¯ï¼ˆåŒ…æ‹¬åˆ°æœŸç§¯åˆ†ï¼‰
    
    å‚æ•°:
        token: Tushare Tokenï¼Œä¸ä¼ åˆ™ä½¿ç”¨get_token()è·å–
        
    è¿”å›:
        åŒ…å«ç§¯åˆ†ä¿¡æ¯çš„å­—å…¸ï¼Œå¦‚æœæŸ¥è¯¢å¤±è´¥è¿”å›None
    """
    try:
        # ä¿®å¤bug: ç»Ÿä¸€ä½¿ç”¨ä¼ å…¥çš„tokenæˆ–é»˜è®¤tokenï¼Œé¿å…ä¸ä¸€è‡´
        actual_token = token or get_token()
        pro = get_pro_client(actual_token)
        df = pro.user(token=actual_token)
        
        if df.empty:
            return None
        
        # è®¡ç®—æ€»ç§¯åˆ†å’Œæœ€è¿‘åˆ°æœŸæ—¶é—´
        total_points = df['åˆ°æœŸç§¯åˆ†'].sum()
        
        # æ‰¾åˆ°æœ€è¿‘çš„åˆ°æœŸæ—¶é—´
        df['åˆ°æœŸæ—¶é—´'] = pd.to_datetime(df['åˆ°æœŸæ—¶é—´'])
        nearest_expiry = df['åˆ°æœŸæ—¶é—´'].min()
        nearest_expiry_points = df[df['åˆ°æœŸæ—¶é—´'] == nearest_expiry]['åˆ°æœŸç§¯åˆ†'].sum()
        
        return {
            'total_points': float(total_points),
            'nearest_expiry_date': nearest_expiry.strftime('%Y-%m-%d') if pd.notna(nearest_expiry) else None,
            'nearest_expiry_points': float(nearest_expiry_points) if pd.notna(nearest_expiry) else 0,
            'expiry_records': df.to_dict('records')
        }
    except Exception as e:
        print(f"æŸ¥è¯¢ç§¯åˆ†ä¿¡æ¯å¤±è´¥: {e}")
        return None


def get_api_delay(api_name: str, user_points: Optional[float] = None, max_workers: int = 1) -> float:
    """
    æ ¹æ®APIåç§°ã€ç”¨æˆ·ç§¯åˆ†ç­‰çº§å’Œå¹¶å‘çº¿ç¨‹æ•°è®¡ç®—åˆé€‚çš„å»¶è¿Ÿæ—¶é—´
    
    é‡è¦è¯´æ˜ï¼š
    - Tushareçš„é¢‘ç‡é™åˆ¶æ˜¯**å…¨å±€é™åˆ¶**ï¼ˆæ‰€æœ‰çº¿ç¨‹åŠ èµ·æ¥çš„æ€»é™åˆ¶ï¼‰
    - è®¡ç®—å…¬å¼ï¼šæ¯ä¸ªçº¿ç¨‹å»¶è¿Ÿ = 60ç§’ / (å…¨å±€é™åˆ¶æ¬¡æ•° / å¹¶å‘çº¿ç¨‹æ•°)
    - ä¾‹å¦‚ï¼š200æ¬¡/åˆ†é’Ÿï¼Œ10çº¿ç¨‹å¹¶å‘ â†’ æ¯ä¸ªçº¿ç¨‹å»¶è¿Ÿ = 60 / (200/10) = 3ç§’
    
    å‚æ•°:
        api_name: APIåç§°ï¼ˆå¦‚ 'stock_company', 'fina_audit', 'balancesheet' ç­‰ï¼‰
        user_points: ç”¨æˆ·ç§¯åˆ†ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨è·å–
        max_workers: å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤1ï¼Œç”¨äºè®¡ç®—æ¯ä¸ªçº¿ç¨‹çš„å»¶è¿Ÿï¼‰
        
    è¿”å›:
        æ¯ä¸ªçº¿ç¨‹çš„å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
    
    APIé¢‘ç‡é™åˆ¶è§„åˆ™ï¼ˆæ ¹æ®Tushareæ–‡æ¡£ï¼Œ2000åˆ†ä¸­çº§ç”¨æˆ·ï¼‰ï¼š
    - stock_company: æ¯åˆ†é’Ÿ10æ¬¡ï¼ˆç‰¹æ®Šé™åˆ¶ï¼Œæ— è®ºç§¯åˆ†ç­‰çº§ï¼‰
    - è´¢åŠ¡æ•°æ®APIï¼ˆfina_audit, balancesheet, income, cashflowï¼‰: 
      * å…è´¹ç”¨æˆ·(0-119åˆ†): æ¯åˆ†é’Ÿ2æ¬¡
      * æ³¨å†Œç”¨æˆ·(120-599åˆ†): æ¯åˆ†é’Ÿ5æ¬¡
      * ä¸­çº§ç”¨æˆ·(600-4999åˆ†): æ¯åˆ†é’Ÿ200æ¬¡ â† 2000åˆ†å±äºè¿™ä¸ªç­‰çº§
      * é«˜çº§ç”¨æˆ·(5000+åˆ†): æ¯åˆ†é’Ÿ200æ¬¡
    - user API: æ¯å¤©50æ¬¡ï¼ˆä¸åœ¨æ­¤å‡½æ•°å¤„ç†ï¼‰
    """
    # å¦‚æœæ²¡æœ‰æä¾›ç§¯åˆ†ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆä¸­çº§ç”¨æˆ·2000åˆ†ï¼‰
    # æ³¨æ„ï¼šä¸å†è°ƒç”¨get_user_points_info()ï¼Œé¿å…é‡å¤è°ƒç”¨API
    # è°ƒç”¨è€…åº”è¯¥åœ¨app.pyçš„main()å‡½æ•°ä¸­è·å–ç§¯åˆ†ä¿¡æ¯å¹¶ä¼ å…¥
    if user_points is None:
        user_points = 2000  # é»˜è®¤ä¸­çº§ç”¨æˆ·ï¼ˆ2000åˆ†ï¼‰
    
    # ç¡®ä¿max_workersè‡³å°‘ä¸º1
    max_workers = max(1, max_workers)
    
    # stock_company APIç‰¹æ®Šé™åˆ¶ï¼šæ¯åˆ†é’Ÿ10æ¬¡ï¼ˆæ— è®ºç§¯åˆ†ç­‰çº§ï¼‰
    if api_name == 'stock_company':
        # å…¨å±€é™åˆ¶ï¼šæ¯åˆ†é’Ÿ10æ¬¡
        # è®¡ç®—å…¬å¼ï¼šæ¯ä¸ªçº¿ç¨‹å»¶è¿Ÿ = 60ç§’ / (10æ¬¡ / çº¿ç¨‹æ•°)
        # å•çº¿ç¨‹ï¼š60 / 10 = 6ç§’
        # 10çº¿ç¨‹ï¼š60 / (10/10) = 60ç§’ï¼ˆå¤ªæ…¢ï¼Œå®é™…å¯ä»¥æ›´æ¿€è¿›ï¼‰
        # æ›´åˆç†çš„ç­–ç•¥ï¼šæ¯ä¸ªçº¿ç¨‹å»¶è¿Ÿ = 60 / 10 * çº¿ç¨‹æ•° / çº¿ç¨‹æ•° = 6ç§’ï¼ˆä¿å®ˆï¼‰
        # æˆ–è€…ï¼š60 / (10 / çº¿ç¨‹æ•°) = 6 * çº¿ç¨‹æ•°ï¼ˆæ›´ä¿å®ˆï¼‰
        # å®é™…ä½¿ç”¨ï¼š6ç§’ï¼ˆæ— è®ºçº¿ç¨‹æ•°ï¼Œå› ä¸ºé™åˆ¶å¾ˆä¸¥æ ¼ï¼‰
        return 6.0  # æ¯åˆ†é’Ÿ10æ¬¡ï¼Œå•çº¿ç¨‹6ç§’ï¼Œå¤šçº¿ç¨‹æ—¶ä¹Ÿä¿æŒ6ç§’ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
    
    # è´¢åŠ¡æ•°æ®APIæ ¹æ®ç§¯åˆ†ç­‰çº§è®¾ç½®å»¶è¿Ÿ
    if api_name in ['fina_audit', 'balancesheet', 'income', 'cashflow', 'fina_indicator']:
        if user_points < 120:
            # å…è´¹ç”¨æˆ·ï¼šæ¯åˆ†é’Ÿ2æ¬¡ï¼ˆå…¨å±€é™åˆ¶ï¼‰
            # 10çº¿ç¨‹å¹¶å‘ï¼š60 / (2/10) = 300ç§’ï¼ˆå¤ªæ…¢ï¼Œå®é™…ä½¿ç”¨ä¿å®ˆç­–ç•¥ï¼‰
            return 30.0  # å•çº¿ç¨‹å»¶è¿Ÿï¼Œå¤šçº¿ç¨‹æ—¶ä¿æŒï¼ˆä¿å®ˆç­–ç•¥ï¼‰
        elif user_points < 600:
            # æ³¨å†Œç”¨æˆ·ï¼šæ¯åˆ†é’Ÿ5æ¬¡ï¼ˆå…¨å±€é™åˆ¶ï¼‰
            return 12.0  # å•çº¿ç¨‹å»¶è¿Ÿ
        elif user_points < 5000:
            # ä¸­çº§ç”¨æˆ·ï¼šæ¯åˆ†é’Ÿ200æ¬¡ï¼ˆå…¨å±€é™åˆ¶ï¼‰â† 2000åˆ†å±äºè¿™ä¸ªç­‰çº§
            # è®¡ç®—å…¬å¼ï¼šæ¯ä¸ªçº¿ç¨‹å»¶è¿Ÿ = 60 / (200 / max_workers)
            # å•çº¿ç¨‹ï¼š60 / 200 = 0.3ç§’
            # 10çº¿ç¨‹ï¼š60 / (200/10) = 3ç§’
            return 60.0 / (200.0 / max_workers) if max_workers > 0 else 0.3
        else:
            # é«˜çº§ç”¨æˆ·ï¼šæ¯åˆ†é’Ÿ200æ¬¡ï¼ˆå…¨å±€é™åˆ¶ï¼‰
            return 60.0 / (200.0 / max_workers) if max_workers > 0 else 0.3
    
    # å…¶ä»–APIé»˜è®¤ä½¿ç”¨ä¸­çº§ç”¨æˆ·çš„å»¶è¿Ÿ
    return 60.0 / (200.0 / max_workers) if max_workers > 0 else 0.3


def run_connectivity_tests(verbose: bool = True) -> Tuple[bool, List[Dict[str, str]]]:
    """
    ç½‘ç»œè¿é€šæ€§ä¸‰é‡æ£€æµ‹
    
    æ£€æµ‹é¡¹ï¼š
        1. DNSè§£æ - æ£€æŸ¥api.waditu.comèƒ½å¦è§£æ
        2. HTTPè¿æ¥ - æ£€æŸ¥HTTPè¯·æ±‚æ˜¯å¦æ­£å¸¸
        3. Tushare API - æ£€æŸ¥APIæ¥å£æ˜¯å¦å¯ç”¨
    
    å‚æ•°:
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—
        
    è¿”å›:
        (æ˜¯å¦å…¨éƒ¨é€šè¿‡, æ—¥å¿—åˆ—è¡¨)
        
    ç”¨é€”ï¼š
        åœ¨æŸ¥è¯¢æ•°æ®å‰é¢„æ£€æŸ¥ç½‘ç»œç¯å¢ƒï¼Œæå‰å‘ç°é—®é¢˜
    """
    checks = [
        ("DNS è¿é€šæ€§", _dns_check),
        ("HTTP æµ‹è¯•", _http_check),
        ("Tushare API", _tushare_check),
    ]
    success = True
    logs: List[Dict[str, str]] = []
    for title, fn in checks:
        ok, message = fn()
        status = "PASS" if ok else "FAIL"
        log_entry = {"status": status, "title": title, "message": message}
        logs.append(log_entry)
        if verbose:
            print(f"[{status}] {title}ï¼š{message}")
        success = success and ok
    return success, logs


def _dns_check() -> Tuple[bool, str]:
    """æ£€æŸ¥åŸŸåèƒ½å¦è§£æã€‚"""
    try:
        ip_addr = socket.gethostbyname(API_HOST)
        return True, f"{API_HOST} -> {ip_addr}"
    except socket.gaierror as exc:
        return False, f"DNS è§£æå¤±è´¥ï¼š{exc}"


def _http_check() -> Tuple[bool, str]:
    """å‘èµ· HTTP è¯·æ±‚éªŒè¯é“¾è·¯ã€‚"""
    try:
        resp = requests.get(f"http://{API_HOST}", timeout=5)
        return True, f"HTTP çŠ¶æ€ {resp.status_code}"
    except requests.RequestException as exc:
        return False, f"HTTP è¯·æ±‚å¤±è´¥ï¼š{exc}"


def _tushare_check() -> Tuple[bool, str]:
    """è°ƒç”¨æœ€å°æ¥å£éªŒè¯ token/ç½‘ç»œæ˜¯å¦æ­£å¸¸ã€‚"""
    try:
        pro = get_pro_client()
        df = pro.trade_cal(limit=1)
        return True, f"trade_cal è¿”å› {len(df)} æ¡è®°å½•"
    except Exception as exc:  # noqa: BLE001
        return False, f"Tushare è°ƒç”¨å¤±è´¥ï¼š{exc}"


@dataclass
class AuditRecord:
    """æ¯ä¸ªæŠ¥å‘ŠæœŸçš„å®¡è®¡æ„è§ã€‚"""

    ann_date: str
    end_date: str
    audit_result: str
    audit_agency: str
    audit_sign: str

    @property
    def is_standard(self) -> bool:
        """æ˜¯å¦ä¸ºæ ‡å‡†æ— ä¿ç•™æ„è§ã€‚"""
        return "æ ‡å‡†æ— ä¿ç•™æ„è§" in (self.audit_result or "")


def fetch_valuation_data(
    ts_code: str,
    trade_date: str,
    target_type: str = "ä¸ªè‚¡",
) -> Optional[Dict[str, Any]]:
    """
    è·å–å¸‚èµšç‡è®¡ç®—æ‰€éœ€çš„ä¼°å€¼æ•°æ®
    
    âš ï¸ é‡è¦ï¼šæ­¤å‡½æ•°ä¸åº”ä½¿ç”¨ç¼“å­˜ï¼
    - ä»·æ ¼æ•°æ®ï¼ˆclose, pe_ttmï¼‰æ¯å¤©å˜åŒ–ï¼Œå¿…é¡»è·å–æœ€æ–°æ•°æ®æ‰èƒ½å‡†ç¡®è®¡ç®—PR
    - å¦‚æœæœªæ¥ä¸ºäº†æ€§èƒ½ä¼˜åŒ–éœ€è¦æ·»åŠ ç¼“å­˜ï¼Œç¼“å­˜æ—¶é—´ä¸åº”è¶…è¿‡1å°æ—¶
    - å»ºè®®ï¼šåœ¨åŒä¸€å¤©å†…å¯ä»¥ç¼“å­˜ï¼Œä½†è·¨å¤©å¿…é¡»é‡æ–°è·å–
    
    å‚æ•°:
        ts_code: è‚¡ç¥¨ä»£ç 
        trade_date: äº¤æ˜“æ—¥æœŸï¼Œæ ¼å¼YYYYMMDD
        target_type: æ ‡çš„ç±»å‹ï¼ˆä¸ªè‚¡/å®½åŸºæŒ‡æ•°ï¼‰
        
    è¿”å›:
        åŒ…å«pe_ttm, roe_waa, eps, dividendç­‰å­—æ®µçš„å­—å…¸
    """
    try:
        pro = get_pro_client()
        
        # 1. è·å–æ¯æ—¥æŒ‡æ ‡ï¼ˆpe_ttm, closeç­‰ï¼‰
        pe_ttm = None
        close_price = None
        data_source = "daily_basic"

        if target_type == "å®½åŸºæŒ‡æ•°":
            index_df = pro.index_dailybasic(
                ts_code=ts_code,
                trade_date=trade_date,
                fields="ts_code,trade_date,close,pe_ttm"
            )
            if index_df.empty and trade_date:
                # è‹¥æŒ‡å®šæ—¥æœŸæ— æ•°æ®ï¼Œå°è¯•è·å–æœ€è¿‘ä¸€æœŸ
                index_df = pro.index_dailybasic(
                    ts_code=ts_code,
                    end_date=trade_date,
                    fields="ts_code,trade_date,close,pe_ttm",
                    limit=1
                )
            if not index_df.empty:
                latest = index_df.sort_values("trade_date", ascending=False).iloc[0]
                pe_ttm = latest.get("pe_ttm")
                close_price = latest.get("close")
                data_source = "index_dailybasic"
        else:
            daily_df = pro.daily_basic(
                ts_code=ts_code,
                trade_date=trade_date,
                fields="ts_code,trade_date,close,pe_ttm"
            )
            if daily_df.empty and trade_date:
                daily_df = pro.daily_basic(
                    ts_code=ts_code,
                    end_date=trade_date,
                    fields="ts_code,trade_date,close,pe_ttm",
                    limit=1
                )
            if not daily_df.empty:
                latest = daily_df.sort_values("trade_date", ascending=False).iloc[0]
                pe_ttm = latest.get("pe_ttm")
                close_price = latest.get("close")
        
        if pe_ttm is None and close_price is None:
            print(f"âš ï¸ æœªè·å–åˆ°{ts_code}åœ¨{trade_date}çš„ä¼°å€¼åŸºç¡€æ•°æ®")
        
        # 2. è·å–è´¢åŠ¡æŒ‡æ ‡ï¼ˆroe_waa, epsï¼‰- è·å–æœ€æ–°çš„è´¢åŠ¡æ•°æ®
        roe_waa = None
        eps = None
        
        if target_type == "ä¸ªè‚¡":
            # å…ˆå°è¯•ä½¿ç”¨æˆªæ­¢åˆ°trade_dateçš„æœ€è¿‘è´¢æŠ¥
            fina_df = pro.fina_indicator(
                ts_code=ts_code,
                end_date=trade_date,
                fields="ts_code,end_date,roe_waa,eps",
                limit=1
            )
            if fina_df.empty:
                # å¦‚æœä»ä¸ºç©ºï¼Œé€€è€Œæ±‚å…¶æ¬¡å–æœ€è¿‘æŠ«éœ²çš„è´¢æŠ¥
                fina_df = pro.fina_indicator(
                    ts_code=ts_code,
                    fields="ts_code,end_date,roe_waa,eps",
                    limit=1
                )
            if not fina_df.empty:
                fina_row = fina_df.sort_values("end_date", ascending=False).iloc[0]
                roe_waa = fina_row.get("roe_waa")
                eps = fina_row.get("eps")
            else:
                print(f"âš ï¸ {ts_code} æœªè·å–åˆ°è´¢åŠ¡æŒ‡æ ‡æ•°æ®ï¼ˆroe_waa / epsï¼‰")
        
        # 3. è·å–åˆ†çº¢æ•°æ® - è·å–æœ€è¿‘ä¸€æ¬¡åˆ†çº¢
        dividend_per_share = None
        if target_type == "ä¸ªè‚¡":
            div_df = pro.dividend(
                ts_code=ts_code,
                end_date=trade_date,
                fields="ts_code,div_proc,cash_div,ex_date,record_date,ann_date,imp_ann_date",
                limit=30
            )
            if div_df.empty:
                div_df = pro.dividend(
                    ts_code=ts_code,
                    fields="ts_code,div_proc,cash_div,ex_date,record_date,ann_date,imp_ann_date",
                    limit=30
                )
            if not div_df.empty:
                div_df = div_df.sort_values("ex_date", ascending=False)
                executed = div_df[
                    (div_df["div_proc"].fillna("") == "å®æ–½") & (div_df["cash_div"].notna()) & (div_df["cash_div"] > 0)
                ]
                source_df = executed if not executed.empty else div_df
                latest_div = source_df.iloc[0]
                dividend_per_share = latest_div.get("cash_div")
        
        return {
            'ts_code': ts_code,
            'trade_date': trade_date,
            'close': close_price,
            'pe_ttm': pe_ttm,
            'roe_waa': roe_waa,
            'eps': eps,
            'dividend_per_share': dividend_per_share,
            'data_source': data_source,
        }
    except Exception as e:
        print(f"è·å–ä¼°å€¼æ•°æ®å¤±è´¥: {e}")
        return None


def fetch_kline_data(ts_code: str, period: str = 'daily', adj: str = 'qfq', limit: int = 500) -> Optional[pd.DataFrame]:
    """
    è·å–Kçº¿æ•°æ® (æ”¯æŒæ—¥/å‘¨/æœˆçº¿åŠå¤æƒ)
    
    å‚æ•°:
        ts_code: è‚¡ç¥¨ä»£ç 
        period: å‘¨æœŸ ('daily', 'weekly', 'monthly')
        adj: å¤æƒç±»å‹ ('qfq'å‰å¤æƒ, 'hfq'åå¤æƒ, Noneä¸å¤æƒ)
        limit: è·å–æ¡æ•°
        
    è¿”å›:
        DataFrame
    """
    try:
        # æ˜ å°„å‘¨æœŸå‚æ•°åˆ° pro_bar çš„ freq å‚æ•°
        # pro_bar freq: D=æ—¥çº¿, W=å‘¨çº¿, M=æœˆçº¿
        freq_map = {'daily': 'D', 'weekly': 'W', 'monthly': 'M'}
        freq = freq_map.get(period, 'D')
        
        # è®¡ç®—å¼€å§‹æ—¥æœŸ (æ ¹æ®limitä¼°ç®—)
        # ä¸ºäº†ä¿è¯MACDè®¡ç®—å‡†ç¡®ï¼Œå¤šå–ä¸€äº›æ•°æ®
        days_per_bar = 1 if freq == 'D' else 5 if freq == 'W' else 20
        total_days = limit * days_per_bar * 2 # å¤šå–ä¸€å€ä»¥é˜²ä¸‡ä¸€
        start_date = (datetime.now() - pd.Timedelta(days=total_days)).strftime("%Y%m%d")
        end_date = datetime.now().strftime("%Y%m%d")
        
        # ä½¿ç”¨ ts.pro_bar è·å–æ•°æ® (è‡ªåŠ¨å¤„ç†å¤æƒå’Œå‘¨æœŸ)
        # æ³¨æ„ï¼šts.pro_bar éœ€è¦åˆå§‹åŒ– pro æ¥å£ï¼Œæˆ–è€…ä¼ å…¥ api å®ä¾‹
        # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨å…¨å±€é…ç½®çš„ token
        
        # ç¡®ä¿ tushare å·²åˆå§‹åŒ–
        token = get_token()
        ts.set_token(token)
        pro = ts.pro_api()
        
        df = ts.pro_bar(
            ts_code=ts_code,
            api=pro,
            adj=adj,
            freq=freq,
            start_date=start_date,
            end_date=end_date
        )
        
        # ---------------------------------------------------------
        # å®æ—¶æ•°æ®æ‹¼æ¥é€»è¾‘
        # ---------------------------------------------------------
        # ä»…åœ¨æ—¥çº¿æ¨¡å¼ä¸‹å°è¯•æ‹¼æ¥å®æ—¶æ•°æ®
        if freq == 'D':
            try:
                # è·å–å®æ—¶è¡Œæƒ… (Sinaæºï¼Œé€Ÿåº¦å¿«)
                # ts_code æ ¼å¼å¦‚ 600519.SHï¼Œget_realtime_quotes éœ€è¦ 600519
                code = ts_code.split('.')[0]
                df_rt = ts.get_realtime_quotes(code)
                
                if df_rt is not None and not df_rt.empty:
                    rt_row = df_rt.iloc[0]
                    rt_date = rt_row['date'] # YYYY-MM-DD
                    rt_date_str = rt_date.replace('-', '') # YYYYMMDD
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‹¼æ¥
                    # å¦‚æœå†å²æ•°æ®ä¸ºç©ºï¼Œæˆ–è€…å†å²æ•°æ®æœ€æ–°æ—¥æœŸå°äºä»Šæ—¥
                    last_date = df['trade_date'].max() if (df is not None and not df.empty) else '00000000'
                    
                    if rt_date_str > last_date:
                        # æ„é€ æ–°è¡Œ
                        # æ³¨æ„ï¼šå®æ—¶æ•°æ®æ˜¯æœªå¤æƒçš„
                        # å¦‚æœ adj='qfq' (å‰å¤æƒ)ï¼Œé€šå¸¸ä»¥å½“å‰ä»·æ ¼ä¸ºåŸºå‡†ï¼Œè¿‡å»ä»·æ ¼å‘ä¸‹è°ƒæ•´ã€‚
                        # æ‰€ä»¥å½“å‰å®æ—¶ä»·æ ¼å¯ä»¥ç›´æ¥ä½œä¸º QFQ ä»·æ ¼ä½¿ç”¨ï¼ˆå› ä¸º QFQ çš„æœ€æ–°ä»· = ç°ä»·ï¼‰ã€‚
                        # å¦‚æœ adj='hfq' (åå¤æƒ)ï¼Œåˆ™éœ€è¦ä¹˜ä»¥å‰é¢çš„å¤æƒå› å­ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨ç°ä»·ï¼ˆå¯èƒ½ä¼šæœ‰æ–­å±‚ï¼Œä½†åœ¨éé™¤æƒæ—¥æ— å½±å“ï¼‰ã€‚
                        
                        new_row = pd.DataFrame([{
                            'ts_code': ts_code,
                            'trade_date': rt_date_str,
                            'open': float(rt_row['open']),
                            'high': float(rt_row['high']),
                            'low': float(rt_row['low']),
                            'close': float(rt_row['price']),
                            'vol': float(rt_row['volume']) / 100, # æ‰‹ -> æ‰‹ (Sinaè¿”å›çš„æ˜¯è‚¡? éœ€ç¡®è®¤. Sina volume is usually in shares, tushare daily vol is in lots (100 shares). Wait, ts.get_realtime_quotes volume is in shares? Let's verify. Usually Sina API returns shares. Tushare daily returns lots. So / 100.)
                            'amount': float(rt_row['amount']) / 1000 # å…ƒ -> åƒå…ƒ
                        }])
                        
                        # æ‹¼æ¥
                        if df is None or df.empty:
                            df = new_row
                        else:
                            df = pd.concat([df, new_row], ignore_index=True)
                            
            except Exception as e:
                print(f"å®æ—¶æ•°æ®æ‹¼æ¥å¤±è´¥: {e}")
        
        if df is None or df.empty:
            print(f"âš ï¸ æœªè·å–åˆ° {ts_code} çš„{period}æ•°æ®")
            return None
            
        # ç»Ÿä¸€åˆ—å (pro_bar è¿”å›çš„åˆ—åé€šå¸¸å·²ç»æ˜¯æ ‡å‡†çš„)
        # ç¡®ä¿æŒ‰æ—¥æœŸå‡åºæ’åˆ—
        df = df.sort_values('trade_date', ascending=True).reset_index(drop=True)
        
        # æˆªå–æœ€è¿‘ limit æ¡
        if len(df) > limit:
            df = df.iloc[-limit:].reset_index(drop=True)
            
        return df
        
    except Exception as e:
        print(f"è·å–Kçº¿æ•°æ®å¤±è´¥: {e}")
        return None


def fetch_company_info(
    ts_code: str,
    use_cache: bool = True,
    return_cache_status: bool = False
) -> Any:
    """
    è·å–å…¬å¸åŸºæœ¬ä¿¡æ¯
    
    ä¼˜åŒ–ï¼šå…¬å¸ä¿¡æ¯å¾ˆå°‘å˜åŒ–ï¼Œä½¿ç”¨30å¤©ç¼“å­˜é¿å…é¢‘ç¹è°ƒç”¨API
    stock_company APIé™åˆ¶ï¼šæ¯åˆ†é’Ÿ10æ¬¡ï¼ˆ2000åˆ†ä¸­çº§ç”¨æˆ·ï¼‰
    
    å‚æ•°:
        ts_code: è‚¡ç¥¨ä»£ç 
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆé»˜è®¤Trueï¼‰
        return_cache_status: æ˜¯å¦è¿”å›ç¼“å­˜å‘½ä¸­çŠ¶æ€ï¼ˆé»˜è®¤Falseï¼‰
        
    è¿”å›:
        å¦‚æœreturn_cache_status=False: å…¬å¸ä¿¡æ¯å­—å…¸ï¼Œå¦‚æœè·å–å¤±è´¥è¿”å›None
        å¦‚æœreturn_cache_status=True: (å…¬å¸ä¿¡æ¯å­—å…¸, æ˜¯å¦å‘½ä¸­ç¼“å­˜)
    """
    from cache_manager import data_cache
    
    # ç”Ÿæˆç¼“å­˜é”®
    cache_key = f"company_info_{ts_code}"
    
    # å…ˆæ£€æŸ¥ç¼“å­˜ï¼ˆ30å¤©æœ‰æ•ˆï¼Œå…¬å¸ä¿¡æ¯å¾ˆå°‘å˜åŒ–ï¼‰
    if use_cache:
        cache_path = data_cache.get_cache_file_path(cache_key)
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸï¼ˆ30å¤© = 2592000ç§’ï¼‰
                cached_time = cache_data.get('timestamp', 0)
                expire_seconds = 30 * 24 * 3600  # 30å¤©
                if time.time() - cached_time < expire_seconds:
                    cached_data = cache_data.get('data')
                    if cached_data is not None:
                        print(f"âœ… ä»ç¼“å­˜åŠ è½½å…¬å¸ä¿¡æ¯ï¼š{ts_code}ï¼ˆ30å¤©ç¼“å­˜æœ‰æ•ˆï¼Œè·³è¿‡APIè°ƒç”¨ï¼‰")
                        if return_cache_status:
                            return cached_data, True
                        return cached_data
                else:
                    # ç¼“å­˜å·²è¿‡æœŸï¼Œåˆ é™¤æ–‡ä»¶
                    try:
                        os.remove(cache_path)
                    except:
                        pass
            except Exception as e:
                # ç¼“å­˜æ–‡ä»¶æŸåï¼Œåˆ é™¤å®ƒ
                print(f"âš ï¸ ç¼“å­˜æ–‡ä»¶æŸåï¼Œåˆ é™¤å¹¶é‡æ–°è·å–ï¼š{e}")
                try:
                    os.remove(cache_path)
                except:
                    pass
    
    # ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨API
    try:
        pro = get_pro_client()
        df = pro.stock_company(
            ts_code=ts_code,
            fields='ts_code,com_name,chairman,manager,secretary,reg_capital,setup_date,province,city,introduction,website,email,employees,main_business,business_scope'
        )
        if df.empty:
            if return_cache_status:
                return None, False
            return None
        
        row = df.iloc[0]
        company_info = {
            'ts_code': row.get('ts_code', ''),
            'com_name': row.get('com_name', ''),
            'chairman': row.get('chairman', ''),
            'manager': row.get('manager', ''),
            'secretary': row.get('secretary', ''),
            'reg_capital': row.get('reg_capital', 0),
            'setup_date': row.get('setup_date', ''),
            'province': row.get('province', ''),
            'city': row.get('city', ''),
            'introduction': row.get('introduction', ''),
            'website': row.get('website', ''),
            'email': row.get('email', ''),
            'employees': row.get('employees', 0),
            'main_business': row.get('main_business', ''),
            'business_scope': row.get('business_scope', ''),
        }
        
        # ä¿å­˜åˆ°ç¼“å­˜ï¼ˆ30å¤©æœ‰æ•ˆï¼‰
        if use_cache:
            # ä½¿ç”¨data_cache.setæ–¹æ³•ï¼Œå®ƒä¼šè‡ªåŠ¨è½¬æ¢numpy/pandasç±»å‹
            # æ³¨æ„ï¼šç”±äºéœ€è¦30å¤©ç¼“å­˜ï¼Œæˆ‘ä»¬ä¸èƒ½ç›´æ¥ä½¿ç”¨data_cache.setï¼ˆå®ƒé»˜è®¤24å°æ—¶ï¼‰
            # æ‰€ä»¥éœ€è¦æ‰‹åŠ¨å¤„ç†ï¼Œä½†è¦ç¡®ä¿ç±»å‹è½¬æ¢
            cache_path = data_cache.get_cache_file_path(cache_key)
            
            # è½¬æ¢numpy/pandasç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
            def convert_to_native(obj):
                """é€’å½’è½¬æ¢numpy/pandasç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
                import numpy as np
                import pandas as pd
                
                if isinstance(obj, (np.integer, np.int64, np.int32)):
                    return int(obj)
                elif isinstance(obj, (np.floating, np.float64, np.float32)):
                    return float(obj) if not pd.isna(obj) else None
                elif isinstance(obj, np.bool_):
                    return bool(obj)
                elif isinstance(obj, pd.Timestamp):
                    return obj.isoformat()
                elif isinstance(obj, dict):
                    return {k: convert_to_native(v) for k, v in obj.items()}
                elif isinstance(obj, (list, tuple)):
                    return [convert_to_native(item) for item in obj]
                elif pd.isna(obj):
                    return None
                else:
                    return obj
            
            # è½¬æ¢å…¬å¸ä¿¡æ¯ä¸­çš„numpyç±»å‹
            company_info_converted = convert_to_native(company_info)
            
            cache_data = {
                'data': company_info_converted,
                'timestamp': time.time(),
                'datetime': datetime.now().isoformat()
            }
            try:
                os.makedirs(os.path.dirname(cache_path), exist_ok=True)
                # å…ˆå†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼ŒæˆåŠŸåå†æ›¿æ¢ï¼ˆé¿å…å¹¶å‘å†™å…¥é—®é¢˜ï¼‰
                temp_path = cache_path + '.tmp'
                with open(temp_path, 'w', encoding='utf-8') as f:
                    json.dump(cache_data, f, ensure_ascii=False, indent=2)
                os.replace(temp_path, cache_path)
                print(f"âœ… å…¬å¸ä¿¡æ¯å·²ç¼“å­˜ï¼ˆ30å¤©æœ‰æ•ˆï¼‰ï¼š{ts_code}")
            except Exception as cache_error:
                print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥ï¼ˆä¸å½±å“ä½¿ç”¨ï¼‰ï¼š{cache_error}")
        
        if return_cache_status:
            return company_info, False
        return company_info
    except Exception as e:
        print(f"è·å–å…¬å¸ä¿¡æ¯å¤±è´¥: {e}")
        # å¦‚æœAPIè°ƒç”¨å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨è¿‡æœŸç¼“å­˜
        if use_cache:
            try:
                cache_path = data_cache.get_cache_file_path(cache_key)
                if os.path.exists(cache_path):
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        cache_data = json.load(f)
                    cached_info = cache_data.get('data')
                    if cached_info:
                        print(f"âš ï¸ ä½¿ç”¨è¿‡æœŸç¼“å­˜çš„å…¬å¸ä¿¡æ¯ï¼š{ts_code}")
                        if return_cache_status:
                            return cached_info, True
                        return cached_info
            except Exception:
                pass
        
        if return_cache_status:
            return None, False
        return None


def fetch_audit_records(
    ts_code: str,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    limit: int = 20,
) -> List[AuditRecord]:
    """è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„å®¡è®¡æ„è§åˆ—è¡¨ã€‚"""
    pro = get_pro_client()
    fields = "ann_date,end_date,audit_result,audit_agency,audit_sign"
    # é‡è¦ï¼šä¸é™åˆ¶æ•°æ®é‡ï¼Œè·å–æ‰€æœ‰å¯ç”¨æ•°æ®
    # å¦‚æœè®¾ç½®äº†æ—¥æœŸèŒƒå›´ï¼Œä½¿ç”¨è¶³å¤Ÿå¤§çš„limitç¡®ä¿è·å–æ‰€æœ‰æ•°æ®
    if start_date and end_date:
        start_year = int(start_date[:4])
        end_year = int(end_date[:4])
        year_span = end_year - start_year + 1
        # æ¯å¹´æœ€å¤š4æ¡ï¼ˆQ1, Q2, Q3, å¹´æŠ¥ï¼‰ï¼Œä¹˜ä»¥å¹´ä»½è·¨åº¦ï¼Œå†åŠ å¤§é‡ç¼“å†²ç¡®ä¿è·å–æ‰€æœ‰æ•°æ®
        api_limit = year_span * 20  # æ¯å¹´20æ¡è®°å½•ï¼ˆè¶³å¤Ÿå¤§çš„ç¼“å†²ï¼Œç¡®ä¿è·å–æ‰€æœ‰æ•°æ®ï¼‰
    else:
        api_limit = 10000  # ä¸é™åˆ¶ï¼Œä½¿ç”¨è¶³å¤Ÿå¤§çš„å€¼ç¡®ä¿è·å–æ‰€æœ‰æ•°æ®
    if start_date and end_date:
        print(f"ğŸ“Š è°ƒç”¨fina_audit APIï¼Œlimit={api_limit}ï¼ˆä¸é™åˆ¶ï¼Œè·å–æ‰€æœ‰æ•°æ®ï¼‰ï¼Œæ—¥æœŸèŒƒå›´ï¼š{start_date} - {end_date}")
    params: Dict[str, Any] = {
        "ts_code": ts_code,
        "start_date": start_date,
        "end_date": end_date,
        "limit": api_limit,
        "fields": fields,
    }
    params = {k: v for k, v in params.items() if v is not None}
    df = pro.fina_audit(**params)
    if df.empty:
        raise ValueError("æœªè·å–åˆ°å®¡è®¡æ„è§ï¼Œè¯·ç¡®è®¤æƒé™æˆ–æŠ«éœ²æƒ…å†µã€‚")
    # ä¸å†ä½¿ç”¨headé™åˆ¶ï¼Œå› ä¸ºå·²ç»é€šè¿‡start_dateå’Œend_dateæ­£ç¡®è¿‡æ»¤äº†
    df = df.sort_values("end_date", ascending=False)
    records = [
        AuditRecord(
            ann_date=row["ann_date"],
            end_date=row["end_date"],
            audit_result=row["audit_result"],
            audit_agency=row["audit_agency"],
            audit_sign=row["audit_sign"],
        )
        for _, row in df.iterrows()
    ]
    return records


def fetch_balancesheet(
    ts_code: str,
    start_date: Optional[str],
    end_date: Optional[str],
    max_records: int,
) -> pd.DataFrame:
    """è·å–èµ„äº§è´Ÿå€ºè¡¨æ•°æ®ã€‚"""
    pro = get_pro_client()
    fields = "ts_code,ann_date,end_date,total_assets,total_liab"
    # é‡è¦ï¼šä¸é™åˆ¶æ•°æ®é‡ï¼Œè·å–æ‰€æœ‰å¯ç”¨æ•°æ®
    # å¦‚æœè®¾ç½®äº†æ—¥æœŸèŒƒå›´ï¼Œä½¿ç”¨è¶³å¤Ÿå¤§çš„limitç¡®ä¿è·å–æ‰€æœ‰æ•°æ®
    if start_date and end_date:
        start_year = int(start_date[:4])
        end_year = int(end_date[:4])
        year_span = end_year - start_year + 1
        # æ¯å¹´æœ€å¤š4æ¡ï¼ˆQ1, Q2, Q3, å¹´æŠ¥ï¼‰ï¼Œä¹˜ä»¥å¹´ä»½è·¨åº¦ï¼Œå†åŠ å¤§é‡ç¼“å†²ç¡®ä¿è·å–æ‰€æœ‰æ•°æ®
        api_limit = year_span * 20  # æ¯å¹´20æ¡è®°å½•ï¼ˆè¶³å¤Ÿå¤§çš„ç¼“å†²ï¼Œç¡®ä¿è·å–æ‰€æœ‰æ•°æ®ï¼‰
    else:
        api_limit = 10000  # ä¸é™åˆ¶ï¼Œä½¿ç”¨è¶³å¤Ÿå¤§çš„å€¼ç¡®ä¿è·å–æ‰€æœ‰æ•°æ®
    print(f"ğŸ“Š è°ƒç”¨balancesheet APIï¼Œlimit={api_limit}ï¼ˆä¸é™åˆ¶ï¼Œè·å–æ‰€æœ‰æ•°æ®ï¼‰ï¼Œæ—¥æœŸèŒƒå›´ï¼š{start_date} - {end_date}")
    df = pro.balancesheet(
        ts_code=ts_code,
        start_date=start_date,
        end_date=end_date,
        fields=fields,
        limit=api_limit,
    )
    return _filter_annual_records(
        df,
        start_date,
        end_date,
        ["total_assets", "total_liab"],
        max_records,
    )


def fetch_income(
    ts_code: str,
    start_date: Optional[str],
    end_date: Optional[str],
    max_records: int,
) -> pd.DataFrame:
    """è·å–åˆ©æ¶¦è¡¨æ•°æ®ã€‚"""
    pro = get_pro_client()
    fields = "ts_code,ann_date,end_date,revenue,oper_cost,n_income"
    # é‡è¦ï¼šä¸é™åˆ¶æ•°æ®é‡ï¼Œè·å–æ‰€æœ‰å¯ç”¨æ•°æ®
    # å¦‚æœè®¾ç½®äº†æ—¥æœŸèŒƒå›´ï¼Œä½¿ç”¨è¶³å¤Ÿå¤§çš„limitç¡®ä¿è·å–æ‰€æœ‰æ•°æ®
    if start_date and end_date:
        start_year = int(start_date[:4])
        end_year = int(end_date[:4])
        year_span = end_year - start_year + 1
        # æ¯å¹´æœ€å¤š4æ¡ï¼ˆQ1, Q2, Q3, å¹´æŠ¥ï¼‰ï¼Œä¹˜ä»¥å¹´ä»½è·¨åº¦ï¼Œå†åŠ å¤§é‡ç¼“å†²ç¡®ä¿è·å–æ‰€æœ‰æ•°æ®
        api_limit = year_span * 20  # æ¯å¹´20æ¡è®°å½•ï¼ˆè¶³å¤Ÿå¤§çš„ç¼“å†²ï¼Œç¡®ä¿è·å–æ‰€æœ‰æ•°æ®ï¼‰
    else:
        api_limit = 10000  # ä¸é™åˆ¶ï¼Œä½¿ç”¨è¶³å¤Ÿå¤§çš„å€¼ç¡®ä¿è·å–æ‰€æœ‰æ•°æ®
    print(f"ğŸ“Š è°ƒç”¨income APIï¼Œlimit={api_limit}ï¼ˆä¸é™åˆ¶ï¼Œè·å–æ‰€æœ‰æ•°æ®ï¼‰ï¼Œæ—¥æœŸèŒƒå›´ï¼š{start_date} - {end_date}")
    df = pro.income(
        ts_code=ts_code,
        start_date=start_date,
        end_date=end_date,
        fields=fields,
        limit=api_limit,
    )
    return _filter_annual_records(
        df,
        start_date,
        end_date,
        ["revenue", "oper_cost", "n_income"],
        max_records,
    )


def fetch_cashflow(
    ts_code: str,
    start_date: Optional[str],
    end_date: Optional[str],
    max_records: int,
) -> pd.DataFrame:
    """è·å–ç°é‡‘æµé‡è¡¨æ•°æ®ã€‚"""
    pro = get_pro_client()
    fields = "ts_code,ann_date,end_date,n_cashflow_act"
    # é‡è¦ï¼šä¸é™åˆ¶æ•°æ®é‡ï¼Œè·å–æ‰€æœ‰å¯ç”¨æ•°æ®
    # å¦‚æœè®¾ç½®äº†æ—¥æœŸèŒƒå›´ï¼Œä½¿ç”¨è¶³å¤Ÿå¤§çš„limitç¡®ä¿è·å–æ‰€æœ‰æ•°æ®
    if start_date and end_date:
        start_year = int(start_date[:4])
        end_year = int(end_date[:4])
        year_span = end_year - start_year + 1
        # æ¯å¹´æœ€å¤š4æ¡ï¼ˆQ1, Q2, Q3, å¹´æŠ¥ï¼‰ï¼Œä¹˜ä»¥å¹´ä»½è·¨åº¦ï¼Œå†åŠ å¤§é‡ç¼“å†²ç¡®ä¿è·å–æ‰€æœ‰æ•°æ®
        api_limit = year_span * 20  # æ¯å¹´20æ¡è®°å½•ï¼ˆè¶³å¤Ÿå¤§çš„ç¼“å†²ï¼Œç¡®ä¿è·å–æ‰€æœ‰æ•°æ®ï¼‰
    else:
        api_limit = 10000  # ä¸é™åˆ¶ï¼Œä½¿ç”¨è¶³å¤Ÿå¤§çš„å€¼ç¡®ä¿è·å–æ‰€æœ‰æ•°æ®
    print(f"ğŸ“Š è°ƒç”¨cashflow APIï¼Œlimit={api_limit}ï¼ˆä¸é™åˆ¶ï¼Œè·å–æ‰€æœ‰æ•°æ®ï¼‰ï¼Œæ—¥æœŸèŒƒå›´ï¼š{start_date} - {end_date}")
    df = pro.cashflow(
        ts_code=ts_code,
        start_date=start_date,
        end_date=end_date,
        fields=fields,
        limit=api_limit,
    )
    return _filter_annual_records(
        df,
        start_date,
        end_date,
        ["n_cashflow_act"],
        max_records,
    )


def calculate_recent_years(required_years: int = 5) -> Tuple[int, int]:
    """
    æ™ºèƒ½è®¡ç®—"æœ€è¿‘Nå¹´"çš„å¹´ä»½èŒƒå›´,è€ƒè™‘å¹´æŠ¥å‘å¸ƒæ—¶é—´
    
    é€»è¾‘:
    - å¹´æŠ¥é€šå¸¸åœ¨æ¬¡å¹´4-5æœˆå‘å¸ƒ
    - å¦‚æœå½“å‰æœˆä»½ < 5æœˆ,ä¸Šä¸€å¹´å¹´æŠ¥å¯èƒ½æœªå‘å¸ƒ,éœ€è¦å¾€å‰æ¨ä¸€å¹´
    - å¦‚æœå½“å‰æœˆä»½ >= 5æœˆ,ä¸Šä¸€å¹´å¹´æŠ¥åº”è¯¥å·²å‘å¸ƒ,å¯ä»¥åŒ…å«
    
    ä¾‹å­:
    - 2026å¹´1æœˆ,éœ€è¦5å¹´: è¿”å› (2020, 2024) - å› ä¸º2025å¹´æŠ¥è¿˜æ²¡å‡º
    - 2026å¹´6æœˆ,éœ€è¦5å¹´: è¿”å› (2021, 2025) - å› ä¸º2025å¹´æŠ¥å·²å‡º
    
    å‚æ•°:
        required_years: éœ€è¦çš„å¹´ä»½æ•°é‡,é»˜è®¤5å¹´
        
    è¿”å›:
        (å¼€å§‹å¹´ä»½, ç»“æŸå¹´ä»½) å…ƒç»„
    """
    current_year = datetime.now().year
    current_month = datetime.now().month
    
    # åˆ¤æ–­ä¸Šä¸€å¹´çš„å¹´æŠ¥æ˜¯å¦å·²å‘å¸ƒ
    if current_month >= 5:
        # 5æœˆåŠä¹‹å,ä¸Šä¸€å¹´å¹´æŠ¥åº”è¯¥å·²å‘å¸ƒ
        end_year = current_year - 1
    else:
        # 1-4æœˆ,ä¸Šä¸€å¹´å¹´æŠ¥å¯èƒ½æœªå‘å¸ƒ,å¾€å‰æ¨ä¸€å¹´
        end_year = current_year - 2
    
    start_year = end_year - required_years + 1
    
    print(f"ğŸ“… æ™ºèƒ½å¹´ä»½è®¡ç®—: å½“å‰{current_year}å¹´{current_month}æœˆ,æœ€è¿‘{required_years}å¹´ = {start_year}-{end_year}")
    
    return start_year, end_year


def analyze_fundamentals(
    ts_code: str,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    years: int = 5,
    use_cache: bool = True,
    api_delay: float = 0.0,  # é¢å¤–å»¶è¿Ÿï¼ˆåœ¨APIè§„åˆ™å»¶è¿ŸåŸºç¡€ä¸Šå¢åŠ ï¼‰
    max_workers: int = 1,  # å¹¶å‘çº¿ç¨‹æ•°ï¼ˆç”¨äºè®¡ç®—åˆé€‚çš„å»¶è¿Ÿï¼‰
    progress_callback=None,
    user_points: Optional[float] = None,  # ç”¨æˆ·ç§¯åˆ†ï¼ˆå¯é€‰ï¼Œé¿å…é‡å¤è°ƒç”¨APIï¼‰
) -> Dict[str, Any]:
    """
    æ‰§è¡Œç»¼åˆåˆ†æï¼Œè®¡ç®—èµ„äº§è´Ÿå€ºç‡ã€æ¯›åˆ©ç‡ã€ç»è¥ç°é‡‘æµç­‰æŒ‡æ ‡ã€‚
    
    âš ï¸ é‡è¦ï¼šæ­¤å‡½æ•°åªç¼“å­˜è´¢åŠ¡æ•°æ®ï¼ˆèµ„äº§è´Ÿå€ºè¡¨ã€åˆ©æ¶¦è¡¨ã€ç°é‡‘æµï¼‰ï¼Œä¸åŒ…å«ä»·æ ¼æ•°æ®ï¼
    - ç¼“å­˜çš„è´¢åŠ¡æ•°æ®ç›¸å¯¹ç¨³å®šï¼ˆå¹´åº¦/å­£åº¦æ›´æ–°ï¼‰ï¼Œå¯ä»¥é•¿æœŸç¼“å­˜ï¼ˆ365å¤©ï¼‰
    - ä¸ç¼“å­˜ä»·æ ¼ã€PEç­‰ä¼°å€¼æ•°æ®ï¼ˆè¿™äº›æ•°æ®æ¯å¤©å˜åŒ–ï¼Œå¿…é¡»å®æ—¶è·å–ï¼‰
    - ä¼°å€¼æ£€æŸ¥åº”ä½¿ç”¨ fetch_valuation_data() è·å–æœ€æ–°ä»·æ ¼æ•°æ®ï¼Œä¸è¦ä½¿ç”¨æ­¤å‡½æ•°çš„ç¼“å­˜æ•°æ®
    
    å‚æ•°:
        ts_code: è‚¡ç¥¨ä»£ç 
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        years: å¹´æ•°
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜

    è¿”å›:
        dict: åŒ…å«å®¡è®¡ä¿¡æ¯ã€æŒ‡æ ‡ DataFrameã€ç°é‡‘æµç»Ÿè®¡ç­‰æ•°æ®ã€‚
        æ³¨æ„ï¼šè¿”å›çš„æ•°æ®ä¸åŒ…å«ä»·æ ¼ã€PEç­‰ä¼°å€¼æ•°æ®ï¼Œè¿™äº›éœ€è¦å•ç‹¬è°ƒç”¨ fetch_valuation_data() è·å–ã€‚
    """
    # ç”Ÿæˆç¼“å­˜é”®ï¼ˆåŒ…å«å®Œæ•´çš„æ—¥æœŸèŒƒå›´ï¼Œç¡®ä¿å¹´ä»½å˜åŒ–æ—¶ç¼“å­˜é”®ä¹Ÿå˜åŒ–ï¼‰
    # ä¿®å¤bug: å¤„ç†Noneå€¼ï¼Œé¿å…ç¼“å­˜é”®å˜æˆ "600519_None_None_5"
    start_date_str = start_date if start_date else 'all'
    end_date_str = end_date if end_date else 'all'
    cache_key = f"{ts_code}_{start_date_str}_{end_date_str}_{years}"
    print(f"ğŸ”‘ ç¼“å­˜é”®ï¼š{cache_key}")
    
    # å¢é‡æ›´æ–°æ ‡å¿—
    incremental_update = False
    cached_base_data = None
    fetch_start_date = start_date
    fetch_end_date = end_date
    
    # å…ˆæ£€æŸ¥ç¼“å­˜
    if use_cache:
        cached_data = data_cache.get(cache_key)
        if cached_data is not None:
            try:
                # å°†cached_dataä¸­çš„DataFrameè½¬å›pandas DataFrame
                if 'metrics_dict' in cached_data and cached_data['metrics_dict']:
                    metrics_df = pd.DataFrame(cached_data['metrics_dict'])
                    
                    # é‡è¦ï¼šä»ç¼“å­˜æ¢å¤æ—¶ï¼Œå¼ºåˆ¶è¿‡æ»¤ï¼Œç¡®ä¿åªä¿ç•™å¹´åº¦æ•°æ®ï¼ˆend_dateä»¥1231ç»“å°¾ï¼‰
                    if not metrics_df.empty:
                        # ç¡®ä¿end_dateæ˜¯å­—ç¬¦ä¸²ç±»å‹
                        metrics_df['end_date'] = metrics_df['end_date'].astype(str)
                        before_filter = len(metrics_df)
                        metrics_df = metrics_df[metrics_df['end_date'].str.endswith('1231')].copy()
                        if before_filter != len(metrics_df):
                            print(f"âš ï¸ ä»ç¼“å­˜æ¢å¤æ—¶è¿‡æ»¤å­£åº¦æ•°æ®ï¼šä»{before_filter}æ¡è®°å½•è¿‡æ»¤åˆ°{len(metrics_df)}æ¡å¹´åº¦è®°å½•ï¼ˆåªä¿ç•™end_dateä»¥1231ç»“å°¾çš„æ•°æ®ï¼‰")
                    
                    # å°†audit_records dictè½¬å›AuditRecordå¯¹è±¡
                    audit_list = []
                    if 'audit_records' in cached_data and isinstance(cached_data['audit_records'], list):
                        audit_list = [
                            AuditRecord(**r) if isinstance(r, dict) else r
                            for r in cached_data['audit_records']
                        ]
                    
                    # é‡æ–°æ„å»ºå®Œæ•´çš„resultå¯¹è±¡
                    result = {
                        'company_info': cached_data.get('company_info'),
                        'metrics': metrics_df,
                        'audit_records': audit_list,
                        'cashflow_positive_years': cached_data.get('cashflow_positive_years', 0),
                        'cashflow_cover_profit': cached_data.get('cashflow_cover_profit', False)
                    }
                    
                    print(f"âœ… ä»ç¼“å­˜åŠ è½½æ•°æ®ï¼š{len(metrics_df)}å¹´æ•°æ®ï¼ˆæ—¥æœŸèŒƒå›´ï¼š{start_date} - {end_date}ï¼‰")
                    # éªŒè¯ç¼“å­˜æ•°æ®çš„å¹´ä»½èŒƒå›´æ˜¯å¦å®Œæ•´
                    if not metrics_df.empty:
                        cached_years = sorted([row['end_date'][:4] for _, row in metrics_df.iterrows()])
                        print(f"ğŸ“… ç¼“å­˜æ•°æ®åŒ…å«çš„å¹´ä»½ï¼š{cached_years}ï¼ˆå·²ç¡®ä¿å…¨éƒ¨ä¸ºå¹´åº¦æ•°æ®ï¼‰")
                    
                    # é‡è¦ï¼šéªŒè¯ç¼“å­˜æ•°æ®çš„å¹´ä»½èŒƒå›´æ˜¯å¦ä¸æŸ¥è¯¢èŒƒå›´åŒ¹é…
                    if start_date and end_date:
                        start_year = int(start_date[:4])
                        end_year = int(end_date[:4])
                        expected_years = list(range(start_year, end_year + 1))
                        actual_years = [int(y) for y in cached_years]
                        missing_years = [y for y in expected_years if y not in actual_years]
                        
                        current_year = datetime.now().year
                        
                        if missing_years:
                            current_month = datetime.now().month
                            
                            # ä¼˜åŒ–ï¼šè¿‡æ»¤æ‰å½“å‰å¹´ä»½ï¼ˆå› ä¸ºå½“å‰å¹´ä»½å¹´æŠ¥è‚¯å®šæ²¡å‡ºï¼Œç¼ºå¤±æ˜¯æ­£å¸¸çš„ï¼‰
                            # ä¾‹å¦‚ï¼šå½“å‰æ˜¯2025å¹´ï¼Œmissing_years=[2024, 2025]ï¼Œæˆ‘ä»¬åªå…³å¿ƒ2024æ˜¯å¦ç¼ºå¤±
                            effective_missing = [y for y in missing_years if y < current_year]
                            
                            # 1. å¦‚æœç¼ºå¤±çš„å¹´ä»½æ˜¯æ›´æ—©çš„å†å²å¹´ä»½ï¼ˆ< current_year - 1ï¼‰ï¼Œå¿…é¡»æ‹’ç»ç¼“å­˜
                            # ä¾‹å¦‚ï¼šå½“å‰2025ï¼Œç¼ºå¤±2023æˆ–æ›´æ—©ï¼Œè¯´æ˜æ•°æ®ä¸¥é‡ä¸å…¨
                            historical_missing = [y for y in effective_missing if y < current_year - 1]
                            
                            if historical_missing:
                                # ===== æ™ºèƒ½å¢é‡æ›´æ–° =====
                                print(f"ğŸ’¡ æ£€æµ‹åˆ°ç¼ºå¤±å†å²å¹´ä»½: {historical_missing}")
                                print(f"ğŸ”„ å¯ç”¨å¢é‡æ›´æ–°: åªè·å–ç¼ºå¤±å¹´ä»½,ä¸åˆ é™¤ç°æœ‰ç¼“å­˜")
                                
                                # è®¡ç®—éœ€è¦è·å–çš„å¹´ä»½èŒƒå›´
                                fetch_start_year = min(historical_missing)
                                fetch_end_year = max(historical_missing)
                                fetch_start_date = f"{fetch_start_year}0101"
                                fetch_end_date = f"{fetch_end_year}1231"
                                
                                print(f"ğŸ“¥ å‡†å¤‡è·å–ç¼ºå¤±å¹´ä»½: {fetch_start_year}-{fetch_end_year}")
                                
                                # è°ƒç”¨APIè·å–ç¼ºå¤±å¹´ä»½çš„æ•°æ®(è¿™éƒ¨åˆ†ä»£ç ä¼šåœ¨åé¢æ‰§è¡Œ)
                                # è®¾ç½®ä¸€ä¸ªæ ‡å¿—,è¡¨ç¤ºéœ€è¦å¢é‡æ›´æ–°
                                incremental_update = True
                                cached_base_data = result  # ä¿å­˜ç°æœ‰ç¼“å­˜æ•°æ®
                                
                                # ä¸return,ç»§ç»­æ‰§è¡Œè·å–æ•°æ®çš„é€»è¾‘
                            
                            # 2. å¦‚æœè¿‡æ»¤åæ²¡æœ‰ç¼ºå¤±å¹´ä»½ï¼ˆå³åªç¼ºå¤±å½“å‰å¹´ä»½ï¼‰ï¼Œæˆ–è€…åªç¼ºå¤±æœ€è¿‘ä¸€å¹´ï¼ˆcurrent_year - 1ï¼‰
                            # æˆ‘ä»¬å…è®¸ä½¿ç”¨ç¼“å­˜ï¼ˆå› ä¸ºæœ€è¿‘ä¸€å¹´å¯èƒ½è¿˜æ²¡å‘å¸ƒï¼Œæˆ–è€…Tushareè¿˜æ²¡æ›´æ–°ï¼‰
                            elif len(effective_missing) <= 1:
                                # åªæœ‰æœ€è¿‘ä¸€å¹´ç¼ºå¤±ï¼Œæˆ–è€…æ²¡æœ‰æœ‰æ•ˆç¼ºå¤±ï¼ˆåªç¼ºå½“å‰å¹´ï¼‰
                                if effective_missing:
                                    missing_year = effective_missing[0]
                                    if current_month >= 5:
                                        print(f"âš ï¸ æ³¨æ„ï¼š{missing_year}å¹´å¹´æŠ¥åº”è¯¥å·²ç»å‘å¸ƒï¼ˆå½“å‰æ˜¯{current_year}å¹´{current_month}æœˆï¼‰ï¼Œä½†Tushareæ•°æ®æºå¯èƒ½è¿˜æ²¡æ›´æ–°")
                                        print(f"ğŸ’¡ ä½¿ç”¨ç°æœ‰ç¼“å­˜æ•°æ®ï¼ˆ{cached_years}ï¼‰ï¼Œå¦‚æœåç»­æ•°æ®æºæ›´æ–°ï¼Œç¼“å­˜ä¼šè‡ªåŠ¨åˆ·æ–°")
                                    else:
                                        print(f"ğŸ’¡ è¯´æ˜ï¼š{missing_year}å¹´å¹´æŠ¥é€šå¸¸åœ¨{current_year}å¹´4-5æœˆå‘å¸ƒï¼Œå½“å‰æ˜¯{current_year}å¹´{current_month}æœˆï¼Œå¯èƒ½è¿˜æœªå‘å¸ƒ")
                                else:
                                    print(f"ğŸ’¡ è¯´æ˜ï¼š{current_year}å¹´å¹´æŠ¥å°šæœªå‘å¸ƒï¼Œä½¿ç”¨ç¼“å­˜æ•°æ®æ˜¯åˆç†çš„")
                                
                                print(f"âš¡ ç¼“å­˜å‘½ä¸­ï¼è·³è¿‡APIè°ƒç”¨ï¼Œç›´æ¥è¿”å›ç¼“å­˜æ•°æ®ï¼ˆèŠ‚çœçº¦6-10ç§’ï¼‰")
                                return result
                                
                            # 3. å¦‚æœè¿‡æ»¤åä»ç¼ºå¤±è¶…è¿‡1å¹´ï¼ˆä¾‹å¦‚ç¼ºå¤±2023, 2024ï¼‰ï¼Œå¯ç”¨å¢é‡æ›´æ–°
                            else:
                                print(f"ğŸ’¡ æ£€æµ‹åˆ°ç¼ºå¤±å¤šä¸ªå¹´ä»½: {effective_missing}")
                                print(f"ğŸ”„ å¯ç”¨å¢é‡æ›´æ–°: åªè·å–ç¼ºå¤±å¹´ä»½{effective_missing},ä¸åˆ é™¤ç°æœ‰ç¼“å­˜{cached_years}")
                                
                                # è®¡ç®—éœ€è¦è·å–çš„å¹´ä»½èŒƒå›´
                                fetch_start_year = min(effective_missing)
                                fetch_end_year = max(effective_missing)
                                fetch_start_date = f"{fetch_start_year}0101"
                                fetch_end_date = f"{fetch_end_year}1231"
                                
                                print(f"ï¿½ å‡†å¤‡è·å–ç¼ºå¤±å¹´ä»½: {fetch_start_year}-{fetch_end_year}")
                                
                                # è®¾ç½®å¢é‡æ›´æ–°æ ‡å¿—
                                incremental_update = True
                                cached_base_data = result
                                
                                # ä¸return,ç»§ç»­æ‰§è¡Œè·å–æ•°æ®çš„é€»è¾‘
                        else:
                            # å¹´ä»½èŒƒå›´å®Œå…¨åŒ¹é…ï¼Œå¯ä»¥ä½¿ç”¨ç¼“å­˜
                            print(f"âš¡ ç¼“å­˜å‘½ä¸­ï¼å¹´ä»½èŒƒå›´å®Œå…¨åŒ¹é…ï¼Œè·³è¿‡APIè°ƒç”¨ï¼ˆèŠ‚çœçº¦6-10ç§’ï¼‰")
                            return result
                    else:
                        # æ²¡æœ‰æŒ‡å®šæ—¥æœŸèŒƒå›´ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜
                        print(f"âš¡ ç¼“å­˜å‘½ä¸­ï¼è·³è¿‡APIè°ƒç”¨ï¼Œç›´æ¥è¿”å›ç¼“å­˜æ•°æ®ï¼ˆèŠ‚çœçº¦6-10ç§’ï¼‰")
                        return result
                else:
                    # ç¼“å­˜æ•°æ®ä¸ºç©ºï¼Œåˆ é™¤å¹¶é‡æ–°è·å–
                    print(f"âš ï¸ è­¦å‘Šï¼šç¼“å­˜æ•°æ®ä¸ºç©ºï¼Œåˆ é™¤å¹¶é‡æ–°è·å–")
                    data_cache.delete(cache_key)
            except Exception as e:
                print(f"âš ï¸ ç¼“å­˜æ•°æ®è§£æå¤±è´¥ï¼Œåˆ é™¤å¹¶é‡æ–°è·å–: {e}")
                data_cache.delete(cache_key)
    
    # ç¼“å­˜æœªå‘½ä¸­æˆ–å¼‚å¸¸ï¼Œè°ƒç”¨APIè·å–æ•°æ®
    if incremental_update:
        print(f"ğŸ”„ å¢é‡æ›´æ–°æ¨¡å¼: åªè·å–ç¼ºå¤±å¹´ä»½çš„æ•°æ® ({fetch_start_date[:4]}-{fetch_end_date[:4]})")
    else:
        print(f"ğŸ”„ ç¼“å­˜æœªå‘½ä¸­ï¼Œå¼€å§‹è°ƒç”¨APIè·å–æ•°æ®...")
    
    # é‡è¦ï¼šä¸é™åˆ¶æ•°æ®é‡ï¼Œè·å–æ‰€æœ‰å¯ç”¨æ•°æ®
    # å¦‚æœæŒ‡å®šäº†æ—¥æœŸèŒƒå›´ï¼Œæ ¹æ®æ—¥æœŸèŒƒå›´è®¡ç®—ï¼Œä½†ä¸è®¾ç½®ä¸Šé™
    if fetch_start_date and fetch_end_date:
        # è®¡ç®—å¹´ä»½è·¨åº¦ï¼ˆä¾‹å¦‚ï¼š19950101 åˆ° 20501231 = 56å¹´ï¼‰
        start_year = int(fetch_start_date[:4])
        end_year = int(fetch_end_date[:4])
        max_records = end_year - start_year + 1
        # ä¸è®¾ç½®ä¸Šé™ï¼Œè·å–æ‰€æœ‰æ•°æ®
        print(f"ğŸ“Š æ ¹æ®æ—¥æœŸèŒƒå›´è®¡ç®—max_recordsï¼š{start_year}-{end_year} = {max_records}å¹´ï¼ˆä¸é™åˆ¶ï¼Œè·å–æ‰€æœ‰æ•°æ®ï¼‰")
    elif start_date or end_date:
        # åªæŒ‡å®šäº†å¼€å§‹æˆ–ç»“æŸæ—¥æœŸï¼Œä½¿ç”¨è¶³å¤Ÿå¤§çš„å€¼ç¡®ä¿è·å–æ‰€æœ‰æ•°æ®
        max_records = 1000  # è¶³å¤Ÿå¤§çš„å€¼ï¼Œç¡®ä¿è·å–æ‰€æœ‰æ•°æ®
    else:
        # æ²¡æœ‰æŒ‡å®šæ—¥æœŸèŒƒå›´ï¼Œä½¿ç”¨yearså‚æ•°ï¼Œä½†ä¸é™åˆ¶
        max_records = max(years, 100)  # è‡³å°‘100å¹´ï¼Œç¡®ä¿è·å–æ‰€æœ‰æ•°æ®
    
    # æ ¹æ®APIè§„åˆ™è‡ªåŠ¨è®¡ç®—å»¶è¿Ÿæ—¶é—´ï¼ˆåŸºäºç”¨æˆ·ç§¯åˆ†ç­‰çº§å’Œå¹¶å‘çº¿ç¨‹æ•°ï¼‰
    # å¦‚æœæ²¡æœ‰ä¼ å…¥user_pointsï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆé¿å…é‡å¤è°ƒç”¨APIï¼‰
    # æ³¨æ„ï¼šè°ƒç”¨è€…åº”è¯¥åœ¨app.pyçš„main()å‡½æ•°ä¸­è·å–ç§¯åˆ†ä¿¡æ¯å¹¶ä¼ å…¥
    if user_points is None:
        user_points = 2000  # é»˜è®¤ä¸­çº§ç”¨æˆ·ï¼ˆ2000åˆ†ï¼‰
    
    # ç¬¬1æ¬¡è°ƒç”¨ï¼šå…¬å¸åŸºæœ¬ä¿¡æ¯ï¼ˆstock_company APIï¼Œæ¯åˆ†é’Ÿ10æ¬¡é™åˆ¶ï¼‰
    if progress_callback:
        progress_callback("æ­£åœ¨è·å–å…¬å¸åŸºæœ¬ä¿¡æ¯... (1/5)", 0.20)
    
    # ä¼˜åŒ–ï¼šè·å–ç¼“å­˜çŠ¶æ€ï¼Œå¦‚æœå‘½ä¸­ç¼“å­˜åˆ™è·³è¿‡å»¶è¿Ÿ
    company_info, is_company_cache_hit = fetch_company_info(ts_code, use_cache=True, return_cache_status=True)
    print(f"âœ… å·²è·å–å…¬å¸ä¿¡æ¯")
    
    # stock_company APIä¸“ç”¨å»¶è¿Ÿï¼ˆæ ¹æ®APIè§„åˆ™å’Œå¹¶å‘çº¿ç¨‹æ•°è‡ªåŠ¨è®¡ç®—ï¼‰
    # åªæœ‰åœ¨æœªå‘½ä¸­ç¼“å­˜æ—¶æ‰éœ€è¦ç­‰å¾…
    if not is_company_cache_hit:
        company_api_delay = get_api_delay('stock_company', user_points, max_workers)
        if api_delay > 0:
            company_api_delay = company_api_delay + api_delay
        print(f"â° ç­‰å¾…{company_api_delay:.2f}ç§’ï¼ˆstock_company APIï¼šæ¯åˆ†é’Ÿ10æ¬¡ï¼Œ{max_workers}çº¿ç¨‹å¹¶å‘ï¼‰...")
        time.sleep(company_api_delay)
    else:
        print(f"âš¡ å…¬å¸ä¿¡æ¯å‘½ä¸­ç¼“å­˜ï¼Œè·³è¿‡APIå»¶è¿Ÿç­‰å¾…")
    
    # ç¬¬2æ¬¡è°ƒç”¨ï¼šå®¡è®¡æ„è§ï¼ˆfina_audit APIï¼‰
    if progress_callback:
        progress_callback("æ­£åœ¨è·å–å®¡è®¡æ„è§... (2/5)", 0.40)
    print(f"ğŸ“… æŸ¥è¯¢æ—¥æœŸèŒƒå›´ï¼šstart_date={fetch_start_date}, end_date={fetch_end_date}")
    audit_records = fetch_audit_records(ts_code, fetch_start_date, fetch_end_date, max_records)
    print(f"âœ… å·²è·å–å®¡è®¡æ„è§ï¼Œå…±{len(audit_records)}æ¡è®°å½•")
    
    # è´¢åŠ¡æ•°æ®APIå»¶è¿Ÿï¼ˆæ ¹æ®ç”¨æˆ·ç§¯åˆ†ç­‰çº§å’Œå¹¶å‘çº¿ç¨‹æ•°è‡ªåŠ¨è®¡ç®—ï¼‰
    financial_api_delay = get_api_delay('fina_audit', user_points, max_workers)
    # api_delayå‚æ•°ä½œä¸ºé¢å¤–å»¶è¿Ÿï¼ˆåœ¨APIè§„åˆ™å»¶è¿ŸåŸºç¡€ä¸Šå¢åŠ ï¼‰
    if api_delay > 0:
        financial_api_delay = financial_api_delay + api_delay
        print(f"â° ç­‰å¾…{financial_api_delay:.2f}ç§’ï¼ˆåŸºç¡€å»¶è¿Ÿ{get_api_delay('fina_audit', user_points, max_workers):.2f}ç§’ + é¢å¤–å»¶è¿Ÿ{api_delay}ç§’ï¼Œ{max_workers}çº¿ç¨‹å¹¶å‘ï¼‰...")
    else:
        print(f"â° ç­‰å¾…{financial_api_delay:.2f}ç§’ï¼ˆè´¢åŠ¡æ•°æ®APIï¼šæ¯åˆ†é’Ÿ200æ¬¡ï¼Œ{user_points:.0f}åˆ†ï¼Œ{max_workers}çº¿ç¨‹å¹¶å‘ï¼‰...")
    
    if financial_api_delay > 0:
        time.sleep(financial_api_delay)
    
    # ç¬¬3æ¬¡è°ƒç”¨ï¼šèµ„äº§è´Ÿå€ºè¡¨ï¼ˆbalancesheet APIï¼‰
    if progress_callback:
        progress_callback("æ­£åœ¨è·å–èµ„äº§è´Ÿå€ºè¡¨... (3/5)", 0.60)
    balance_df = fetch_balancesheet(ts_code, fetch_start_date, fetch_end_date, max_records)
    print(f"âœ… å·²è·å–èµ„äº§è´Ÿå€ºè¡¨")
    
    if financial_api_delay > 0:
        time.sleep(financial_api_delay)
    
    # ç¬¬4æ¬¡è°ƒç”¨ï¼šåˆ©æ¶¦è¡¨ï¼ˆincome APIï¼‰
    if progress_callback:
        progress_callback("æ­£åœ¨è·å–åˆ©æ¶¦è¡¨... (4/5)", 0.80)
    income_df = fetch_income(ts_code, fetch_start_date, fetch_end_date, max_records)
    print(f"âœ… å·²è·å–åˆ©æ¶¦è¡¨")
    
    if financial_api_delay > 0:
        time.sleep(financial_api_delay)
    
    # ç¬¬5æ¬¡è°ƒç”¨ï¼šç°é‡‘æµé‡è¡¨ï¼ˆcashflow APIï¼‰
    if progress_callback:
        progress_callback("æ­£åœ¨è·å–ç°é‡‘æµé‡è¡¨... (5/5)", 1.0)
    cashflow_df = fetch_cashflow(ts_code, fetch_start_date, fetch_end_date, max_records)
    print("âœ… å·²è·å–ç°é‡‘æµé‡è¡¨ï¼Œæ•°æ®æ”¶é›†å®Œæˆï¼")
    print(f"ğŸ“Š è·å–åˆ°çš„åŸå§‹æ•°æ®ç»Ÿè®¡ï¼š")
    if not balance_df.empty:
        balance_years = sorted([row['end_date'][:4] for _, row in balance_df.iterrows()])
        print(f"  - èµ„äº§è´Ÿå€ºè¡¨ï¼š{len(balance_df)}æ¡è®°å½•ï¼Œå¹´ä»½èŒƒå›´ï¼š{balance_years[0] if balance_years else 'N/A'} - {balance_years[-1] if balance_years else 'N/A'}ï¼Œå¹´ä»½åˆ—è¡¨ï¼š{balance_years}")
    if not income_df.empty:
        income_years = sorted([row['end_date'][:4] for _, row in income_df.iterrows()])
        print(f"  - åˆ©æ¶¦è¡¨ï¼š{len(income_df)}æ¡è®°å½•ï¼Œå¹´ä»½èŒƒå›´ï¼š{income_years[0] if income_years else 'N/A'} - {income_years[-1] if income_years else 'N/A'}ï¼Œå¹´ä»½åˆ—è¡¨ï¼š{income_years}")
    if not cashflow_df.empty:
        cashflow_years = sorted([row['end_date'][:4] for _, row in cashflow_df.iterrows()])
        print(f"  - ç°é‡‘æµé‡è¡¨ï¼š{len(cashflow_df)}æ¡è®°å½•ï¼Œå¹´ä»½èŒƒå›´ï¼š{cashflow_years[0] if cashflow_years else 'N/A'} - {cashflow_years[-1] if cashflow_years else 'N/A'}ï¼Œå¹´ä»½åˆ—è¡¨ï¼š{cashflow_years}")

    # æ•°æ®åˆå¹¶ï¼šä½¿ç”¨inner joinç¡®ä¿ä¸‰ä¸ªè¡¨éƒ½æœ‰æ•°æ®çš„å¹´ä»½æ‰ä¿ç•™
    # æ³¨æ„ï¼šå¦‚æœæŸä¸ªå¹´ä»½æŸä¸ªè¡¨æ•°æ®ç¼ºå¤±ï¼Œè¯¥å¹´ä»½ä¼šè¢«è¿‡æ»¤æ‰
    print(f"ğŸ” åˆå¹¶å‰æ•°æ®ç»Ÿè®¡ï¼š")
    if not balance_df.empty:
        balance_years = sorted([row['end_date'][:4] for _, row in balance_df.iterrows()])
        print(f"  - èµ„äº§è´Ÿå€ºè¡¨ï¼š{len(balance_df)}æ¡ï¼Œå¹´ä»½ï¼š{balance_years}")
    else:
        print(f"  - èµ„äº§è´Ÿå€ºè¡¨ï¼š{len(balance_df)}æ¡ï¼ˆç©ºï¼‰")
    if not income_df.empty:
        income_years = sorted([row['end_date'][:4] for _, row in income_df.iterrows()])
        print(f"  - åˆ©æ¶¦è¡¨ï¼š{len(income_df)}æ¡ï¼Œå¹´ä»½ï¼š{income_years}")
    else:
        print(f"  - åˆ©æ¶¦è¡¨ï¼š{len(income_df)}æ¡ï¼ˆç©ºï¼‰")
    if not cashflow_df.empty:
        cashflow_years = sorted([row['end_date'][:4] for _, row in cashflow_df.iterrows()])
        print(f"  - ç°é‡‘æµé‡è¡¨ï¼š{len(cashflow_df)}æ¡ï¼Œå¹´ä»½ï¼š{cashflow_years}")
    else:
        print(f"  - ç°é‡‘æµé‡è¡¨ï¼š{len(cashflow_df)}æ¡ï¼ˆç©ºï¼‰")
    
    # é‡è¦ï¼šæ£€æŸ¥åˆå¹¶å‰çš„æ•°æ®å®Œæ•´æ€§
    # å¦‚æœæŸä¸ªè¡¨æ•°æ®ä¸å®Œæ•´ï¼Œinner joinä¼šå¯¼è‡´æ•°æ®ä¸¢å¤±
    balance_years_set = set([row['end_date'][:4] for _, row in balance_df.iterrows()]) if not balance_df.empty else set()
    income_years_set = set([row['end_date'][:4] for _, row in income_df.iterrows()]) if not income_df.empty else set()
    cashflow_years_set = set([row['end_date'][:4] for _, row in cashflow_df.iterrows()]) if not cashflow_df.empty else set()
    
    # è®¡ç®—äº¤é›†ï¼šèµ„äº§è´Ÿå€ºè¡¨å’Œåˆ©æ¶¦è¡¨çš„äº¤é›†ï¼ˆinner joinåï¼‰
    common_years = balance_years_set & income_years_set
    print(f"ğŸ” åˆå¹¶åˆ†æï¼š")
    print(f"  - èµ„äº§è´Ÿå€ºè¡¨å¹´ä»½ï¼š{sorted(balance_years_set)}")
    print(f"  - åˆ©æ¶¦è¡¨å¹´ä»½ï¼š{sorted(income_years_set)}")
    print(f"  - ç°é‡‘æµé‡è¡¨å¹´ä»½ï¼š{sorted(cashflow_years_set)}")
    print(f"  - èµ„äº§è´Ÿå€ºè¡¨å’Œåˆ©æ¶¦è¡¨çš„äº¤é›†ï¼ˆinner joinåï¼‰ï¼š{sorted(common_years)}")
    
    # å¦‚æœäº¤é›†å°‘äºæœŸæœ›ï¼Œç»™å‡ºè­¦å‘Š
    if start_date and end_date:
        expected_years = set([str(y) for y in range(int(start_date[:4]), int(end_date[:4]) + 1)])
        missing_in_balance = expected_years - balance_years_set
        missing_in_income = expected_years - income_years_set
        if missing_in_balance:
            print(f"âš ï¸ è­¦å‘Šï¼šèµ„äº§è´Ÿå€ºè¡¨ç¼ºå¤±å¹´ä»½ï¼š{sorted(missing_in_balance)}")
        if missing_in_income:
            print(f"âš ï¸ è­¦å‘Šï¼šåˆ©æ¶¦è¡¨ç¼ºå¤±å¹´ä»½ï¼š{sorted(missing_in_income)}")
        if missing_in_balance or missing_in_income:
            print(f"âš ï¸ æ³¨æ„ï¼šinner joinä¼šå¯¼è‡´ç¼ºå¤±çš„å¹´ä»½è¢«è¿‡æ»¤æ‰ï¼Œæœ€ç»ˆåªæœ‰äº¤é›†å¹´ä»½ï¼š{sorted(common_years)}")
    
    merged = (
        balance_df[["end_date", "total_assets", "total_liab"]]
        .merge(
            income_df[["end_date", "revenue", "oper_cost", "n_income"]],
            on="end_date",
            how="inner",  # inner joinï¼šåªä¿ç•™ä¸¤ä¸ªè¡¨éƒ½æœ‰çš„end_date
        )
        .merge(
            cashflow_df[["end_date", "n_cashflow_act"]],
            on="end_date",
            how="left",  # left joinï¼šä¿ç•™å‰ä¸¤ä¸ªè¡¨åˆå¹¶åçš„æ‰€æœ‰end_dateï¼Œå³ä½¿ç°é‡‘æµé‡è¡¨ç¼ºå¤±
        )
        .sort_values("end_date", ascending=False)
        .reset_index(drop=True)
    )
    
    # é‡è¦ä¿®å¤ï¼šåˆå¹¶åå†æ¬¡è¿‡æ»¤ï¼Œç¡®ä¿åªä¿ç•™å¹´åº¦æ•°æ®ï¼ˆend_dateä»¥1231ç»“å°¾ï¼‰
    # è™½ç„¶æ¯ä¸ªè¡¨éƒ½å·²ç»è¿‡æ»¤äº†ï¼Œä½†ä¸ºäº†ä¿é™©èµ·è§ï¼Œåˆå¹¶åå†æ¬¡è¿‡æ»¤
    if not merged.empty:
        before_filter_count = len(merged)
        merged = merged[merged['end_date'].str.endswith('1231')].copy()
        after_filter_count = len(merged)
        if before_filter_count != after_filter_count:
            print(f"âš ï¸ åˆå¹¶åè¿‡æ»¤ï¼šä»{before_filter_count}æ¡è®°å½•è¿‡æ»¤åˆ°{after_filter_count}æ¡å¹´åº¦è®°å½•ï¼ˆå·²è¿‡æ»¤æ‰å­£åº¦æ•°æ®ï¼‰")
    
    print(f"ğŸ” åˆå¹¶åæ•°æ®ç»Ÿè®¡ï¼š{len(merged)}æ¡è®°å½•ï¼ˆå·²ç¡®ä¿å…¨éƒ¨ä¸ºå¹´åº¦æ•°æ®ï¼‰")
    
    # æ‰“å°åˆå¹¶åçš„æ•°æ®å¹´ä»½èŒƒå›´ï¼ˆæ™ºèƒ½åˆ¤æ–­ï¼Œé¿å…ä¸å¿…è¦çš„è­¦å‘Šï¼‰
    if not merged.empty:
        merged_years = sorted([row['end_date'][:4] for _, row in merged.iterrows()])
        print(f"ğŸ“… åˆå¹¶åçš„æ•°æ®å¹´ä»½ï¼š{merged_years}ï¼ˆå…±{len(merged)}å¹´ï¼‰")
        
        # è®¡ç®—æœŸæœ›çš„å¹´ä»½èŒƒå›´
        if start_date and end_date:
            start_year = int(start_date[:4])
            end_year = int(end_date[:4])
            expected_years = list(range(start_year, end_year + 1))
            actual_years = [int(y) for y in merged_years]
            missing_years = [y for y in expected_years if y not in actual_years]
            
            current_year = datetime.now().year
            current_month = datetime.now().month
            print(f"ğŸ“… æŸ¥è¯¢å¹´ä»½èŒƒå›´ï¼š{start_year}-{end_year}å¹´ | å®é™…è¿”å›æ•°æ®å¹´ä»½ï¼š{merged_years}ï¼ˆå…±{len(merged)}å¹´ï¼‰")
            
            # å¦‚æœæ•°æ®å°‘äºæœŸæœ›ï¼Œæ‰“å°è¯¦ç»†è°ƒè¯•ä¿¡æ¯
            if len(merged) < (end_year - start_year + 1):
                print(f"âš ï¸ æ•°æ®ä¸å®Œæ•´ï¼šæœŸæœ›{end_year - start_year + 1}å¹´ï¼Œå®é™…{len(merged)}å¹´")
                if missing_years:
                    print(f"   ç¼ºå¤±å¹´ä»½ï¼š{missing_years}")
                print(f"   å¯èƒ½åŸå› ï¼šæŸäº›å¹´ä»½çš„èµ„äº§è´Ÿå€ºè¡¨æˆ–åˆ©æ¶¦è¡¨æ•°æ®ç¼ºå¤±ï¼Œå¯¼è‡´inner joinè¿‡æ»¤æ‰äº†")
            
            if missing_years:
                # å¦‚æœç¼ºå¤±çš„å¹´ä»½æ˜¯å½“å‰å¹´ä»½æˆ–æœªæ¥å¹´ä»½ï¼Œè¯´æ˜å¹´æŠ¥è¿˜æœªå‘å¸ƒï¼Œè¿™æ˜¯æ­£å¸¸çš„
                if all(y >= current_year for y in missing_years):
                    print(f"ğŸ’¡ è¯´æ˜ï¼š{missing_years}å¹´çš„å¹´æŠ¥å°šæœªå‘å¸ƒï¼ˆé€šå¸¸åœ¨æ¬¡å¹´4-5æœˆå‘å¸ƒï¼‰ï¼Œè¿™æ˜¯æ­£å¸¸æƒ…å†µ")
                elif len(missing_years) > 1 or (missing_years and missing_years[0] < current_year - 1):
                    print(f"âš ï¸ è­¦å‘Šï¼šç¼ºå°‘ä»¥ä¸‹å¹´ä»½çš„æ•°æ®ï¼š{missing_years}ï¼ˆå¯èƒ½æ˜¯æ•°æ®ç¼ºå¤±ï¼‰")
                # ç‰¹æ®Šæƒ…å†µï¼šå¦‚æœç¼ºå¤±çš„æ˜¯ end_year - 1ï¼ˆæŸ¥è¯¢çš„æœ€åä¸€å¹´ï¼‰ï¼Œä¸” end_year = current_yearï¼Œä¸”å½“å‰æœˆä»½ >= 5
                # è¯´æ˜å¹´æŠ¥åº”è¯¥å·²ç»å‘å¸ƒäº†ï¼Œä½†æ•°æ®æºå¯èƒ½è¿˜æ²¡æ›´æ–°
                elif len(missing_years) == 1 and missing_years[0] == end_year - 1 and end_year == current_year:
                    if current_month >= 5:
                        print(f"âš ï¸ æ³¨æ„ï¼š{missing_years[0]}å¹´å¹´æŠ¥åº”è¯¥å·²ç»å‘å¸ƒï¼ˆå½“å‰æ˜¯{current_year}å¹´{current_month}æœˆï¼‰ï¼Œä½†Tushareæ•°æ®æºå¯èƒ½è¿˜æ²¡æ›´æ–°")
                        print(f"ğŸ’¡ å»ºè®®ï¼šå¯ä»¥ç¨åå†è¯•ï¼Œæˆ–è€…æ£€æŸ¥è¯¥è‚¡ç¥¨æ˜¯å¦æœ‰2024å¹´å¹´æŠ¥æ•°æ®")
                    else:
                        print(f"ğŸ’¡ è¯´æ˜ï¼š{missing_years[0]}å¹´å¹´æŠ¥é€šå¸¸åœ¨{current_year}å¹´4-5æœˆå‘å¸ƒï¼Œå½“å‰æ˜¯{current_year}å¹´{current_month}æœˆï¼Œå¯èƒ½è¿˜æœªå‘å¸ƒ")
        else:
            print(f"ğŸ“… åˆå¹¶åçš„æ•°æ®å¹´ä»½ï¼š{merged_years}ï¼ˆå…±{len(merged)}å¹´ï¼‰")
    else:
        print(f"âš ï¸ è­¦å‘Šï¼šåˆå¹¶åçš„æ•°æ®ä¸ºç©ºï¼")

    # ä¿®å¤bug: æ·»åŠ é™¤é›¶æ£€æŸ¥å’Œç©ºå€¼éªŒè¯ï¼Œé¿å…è®¡ç®—é”™è¯¯
    def safe_calc_debt_ratio(row):
        """å®‰å…¨è®¡ç®—èµ„äº§è´Ÿå€ºç‡ï¼Œå¤„ç†é™¤é›¶å’Œç©ºå€¼"""
        total_liab = row.get('total_liab', 0)
        total_assets = row.get('total_assets', 0)
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºå€¼
        if pd.isna(total_liab) or pd.isna(total_assets):
            return pd.NA
        # æ£€æŸ¥é™¤é›¶æƒ…å†µ
        if total_assets == 0:
            return pd.NA
        return total_liab / total_assets
    
    merged["debt_ratio"] = merged.apply(safe_calc_debt_ratio, axis=1)
    
    # è®¡ç®—æ¯›åˆ©ç‡ï¼šéœ€è¦æ£€æŸ¥revenueæ˜¯å¦ä¸º0æˆ–NaN
    # å¦‚æœrevenueä¸º0æˆ–NaNï¼Œåˆ™æ¯›åˆ©ç‡ä¸ºNaNï¼ˆè¡¨ç¤ºæ•°æ®ç¼ºå¤±ï¼‰
    def calc_gross_margin(row):
        revenue = row.get('revenue', 0)
        oper_cost = row.get('oper_cost', 0)
        if pd.isna(revenue) or revenue == 0:
            return pd.NA  # è¿”å›NaNè¡¨ç¤ºæ•°æ®ç¼ºå¤±
        if pd.isna(oper_cost):
            return pd.NA  # å¦‚æœæˆæœ¬ç¼ºå¤±ï¼Œä¹Ÿæ— æ³•è®¡ç®—æ¯›åˆ©ç‡
        return (revenue - oper_cost) / revenue
    
    merged["gross_margin"] = merged.apply(calc_gross_margin, axis=1)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ¯›åˆ©ç‡ç¼ºå¤±çš„æƒ…å†µï¼Œå¹¶æ‰“å°è°ƒè¯•ä¿¡æ¯
    missing_gross_margin = merged["gross_margin"].isna().sum()
    if missing_gross_margin > 0:
        print(f"âš ï¸ è­¦å‘Šï¼šæœ‰ {missing_gross_margin} å¹´çš„æ¯›åˆ©ç‡æ•°æ®ç¼ºå¤±ï¼ˆå¯èƒ½æ˜¯è´¢æŠ¥ä¸­revenueæˆ–oper_costå­—æ®µç¼ºå¤±ï¼‰")
        # æ‰“å°ç¼ºå¤±çš„å¹´ä»½å’ŒåŸå› 
        for idx, row in merged[merged["gross_margin"].isna()].iterrows():
            revenue = row.get('revenue', 0)
            oper_cost = row.get('oper_cost', 0)
            year = row['end_date'][:4] if pd.notna(row.get('end_date')) else 'æœªçŸ¥'
            if pd.isna(revenue) or revenue == 0:
                print(f"  - {year}å¹´ï¼šrevenueç¼ºå¤±æˆ–ä¸º0 (revenue={revenue})")
            elif pd.isna(oper_cost):
                print(f"  - {year}å¹´ï¼šoper_costç¼ºå¤± (oper_cost={oper_cost})")
    
    merged["cashflow_positive"] = merged["n_cashflow_act"] >= 0
    merged["cashflow_ge_profit"] = merged["n_cashflow_act"] >= merged["n_income"]

    result = {
        "company_info": company_info,
        "audit_records": audit_records,
        "metrics": merged,
        "cashflow_positive_years": int(merged["cashflow_positive"].sum()),
        "cashflow_cover_profit": bool(merged["cashflow_ge_profit"].all()),
    }
    
    # ===== å¢é‡æ›´æ–°ï¼šåˆå¹¶æ–°æ—§æ•°æ® =====
    if incremental_update and cached_base_data is not None:
        print(f"ğŸ”„ æ‰§è¡Œå¢é‡æ›´æ–°: åˆå¹¶æ–°æ•°æ®ä¸ç°æœ‰ç¼“å­˜")
        
        # è·å–ç°æœ‰ç¼“å­˜çš„metrics
        cached_metrics = cached_base_data.get('metrics', pd.DataFrame())
        
        if not cached_metrics.empty:
            print(f"ğŸ“Š ç°æœ‰ç¼“å­˜æ•°æ®: {len(cached_metrics)} å¹´")
            print(f"ğŸ“Š æ–°è·å–æ•°æ®: {len(merged)} å¹´")
            
            # åˆå¹¶æ•°æ®ï¼šç°æœ‰ç¼“å­˜ + æ–°è·å–çš„æ•°æ®
            combined_metrics = pd.concat([cached_metrics, merged], ignore_index=True)
            
            # å»é‡ï¼šå¦‚æœåŒä¸€å¹´ä»½åŒæ—¶å­˜åœ¨äºç¼“å­˜å’Œæ–°æ•°æ®ä¸­,ä¿ç•™æ–°æ•°æ®
            # æŒ‰end_dateæ’åºåå»é‡,ä¿ç•™æœ€åå‡ºç°çš„(å³æ–°æ•°æ®)
            combined_metrics = combined_metrics.sort_values('end_date').drop_duplicates(subset=['end_date'], keep='last')
            combined_metrics = combined_metrics.sort_values('end_date', ascending=False).reset_index(drop=True)
            
            # ç¡®ä¿åªä¿ç•™å¹´åº¦æ•°æ®
            combined_metrics = combined_metrics[combined_metrics['end_date'].str.endswith('1231')].copy()
            
            combined_years = sorted([row['end_date'][:4] for _, row in combined_metrics.iterrows()])
            print(f"âœ… åˆå¹¶å®Œæˆ: å…± {len(combined_metrics)} å¹´æ•°æ®, å¹´ä»½={combined_years}")
            
            # æ›¿æ¢ä¸ºåˆå¹¶åçš„æ•°æ®
            merged = combined_metrics
            
            # é‡æ–°è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            result['metrics'] = merged
            result['cashflow_positive_years'] = int(merged["cashflow_positive"].sum())
            result['cashflow_cover_profit'] = bool(merged["cashflow_ge_profit"].all())
            
            # åˆå¹¶å®¡è®¡è®°å½•
            cached_audits = cached_base_data.get('audit_records', [])
            if cached_audits:
                # å°†æ–°æ—§å®¡è®¡è®°å½•åˆå¹¶å¹¶å»é‡
                all_audits = audit_records + cached_audits
                # æŒ‰end_dateå»é‡,ä¿ç•™æ–°è®°å½•
                seen_dates = set()
                unique_audits = []
                for audit in sorted(all_audits, key=lambda x: x.end_date, reverse=True):
                    if audit.end_date not in seen_dates:
                        unique_audits.append(audit)
                        seen_dates.add(audit.end_date)
                audit_records = unique_audits
                result['audit_records'] = audit_records
                print(f"âœ… å®¡è®¡è®°å½•åˆå¹¶å®Œæˆ: å…± {len(audit_records)} æ¡")
    
    # ä¿å­˜åˆ°ç¼“å­˜
    if use_cache:
        # é‡è¦ï¼šä¿å­˜å‰å†æ¬¡ç¡®è®¤åªä¿å­˜å¹´åº¦æ•°æ®ï¼ˆend_dateä»¥1231ç»“å°¾ï¼‰
        # è™½ç„¶mergedå·²ç»è¿‡æ»¤è¿‡äº†ï¼Œä½†ä¸ºäº†ä¿é™©èµ·è§ï¼Œå†æ¬¡ç¡®è®¤
        merged_for_cache = merged[merged['end_date'].str.endswith('1231')].copy()
        if len(merged_for_cache) != len(merged):
            print(f"âš ï¸ ä¿å­˜ç¼“å­˜å‰è¿‡æ»¤ï¼šä»{len(merged)}æ¡è®°å½•è¿‡æ»¤åˆ°{len(merged_for_cache)}æ¡å¹´åº¦è®°å½•")
            merged = merged_for_cache
        
        # å‡†å¤‡ç¼“å­˜æ•°æ®
        # æ³¨æ„ï¼šdata_cache.setä¼šè‡ªåŠ¨å¤„ç†numpyç±»å‹çš„è½¬æ¢
        cache_data = {
            'company_info': company_info,
            'metrics_dict': merged.to_dict('records'),
            'cashflow_positive_years': int(merged["cashflow_positive"].sum()),
            'cashflow_cover_profit': bool(merged["cashflow_ge_profit"].all()),
            'audit_records': [
                {
                    'ann_date': r.ann_date,
                    'end_date': r.end_date,
                    'audit_result': r.audit_result,
                    'audit_agency': r.audit_agency,
                    'audit_sign': r.audit_sign,
                }
                for r in audit_records
            ]
        }
        
        saved = data_cache.set(cache_key, cache_data)
        if saved:
            print(f"âœ… æ•°æ®å·²ç¼“å­˜ï¼š{cache_key}")
        else:
            print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥")
    
    return result


def _filter_annual_records(
    df: pd.DataFrame,
    start_date: Optional[str],
    end_date: Optional[str],
    value_columns: List[str],
    max_records: int,
) -> pd.DataFrame:
    """ç­›é€‰å¹´æŠ¥å¹¶è½¬æ¢å­—æ®µç±»å‹ã€‚"""
    if df.empty:
        raise ValueError("æ¥å£è¿”å›ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ ts_code æˆ–æƒé™ã€‚")

    # æ‰“å°åŸå§‹æ•°æ®ç»Ÿè®¡ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    if start_date and end_date:
        all_end_dates = df["end_date"].astype(str).tolist() if "end_date" in df.columns else []
        annual_dates = [d for d in all_end_dates if str(d).endswith("1231")]
        if len(annual_dates) != len(all_end_dates):
            print(f"ğŸ“Š APIè¿”å›åŸå§‹æ•°æ®ï¼šå…±{len(all_end_dates)}æ¡è®°å½•ï¼Œå…¶ä¸­å¹´æŠ¥{len(annual_dates)}æ¡ï¼ˆend_dateä»¥1231ç»“å°¾ï¼‰")
            if annual_dates:
                annual_years = sorted([str(d)[:4] for d in annual_dates])
                print(f"ğŸ“… åŸå§‹æ•°æ®ä¸­çš„å¹´æŠ¥å¹´ä»½ï¼š{annual_years}")

    df["end_date"] = df["end_date"].astype(str)
    # å¼ºåˆ¶è¿‡æ»¤ï¼šåªä¿ç•™å¹´åº¦æ•°æ®ï¼ˆend_dateå¿…é¡»ä»¥1231ç»“å°¾ï¼Œä¾‹å¦‚ï¼š20231231ï¼‰
    # å­£åº¦æ•°æ®æ ¼å¼ï¼š20230331ï¼ˆQ1ï¼‰ã€20230630ï¼ˆQ2ï¼‰ã€20230930ï¼ˆQ3ï¼‰ã€20231231ï¼ˆå¹´æŠ¥ï¼‰
    # åªæœ‰1231ç»“å°¾çš„æ‰æ˜¯å¹´åº¦æ•°æ®
    before_filter = len(df)
    df = df[df["end_date"].str.endswith("1231")].copy()
    if before_filter != len(df):
        print(f"ğŸ“Š è¿‡æ»¤å­£åº¦æ•°æ®ï¼šä»{before_filter}æ¡è®°å½•è¿‡æ»¤åˆ°{len(df)}æ¡å¹´åº¦è®°å½•ï¼ˆåªä¿ç•™end_dateä»¥1231ç»“å°¾çš„æ•°æ®ï¼‰")
    
    # é‡è¦è°ƒè¯•ï¼šæ‰“å°è¿‡æ»¤åçš„æ•°æ®å¹´ä»½
    if not df.empty:
        filtered_years = sorted([row['end_date'][:4] for _, row in df.iterrows()])
        print(f"ğŸ“… è¿‡æ»¤åçš„å¹´åº¦æ•°æ®å¹´ä»½ï¼š{filtered_years}ï¼ˆå…±{len(df)}å¹´ï¼‰")
    
    if df.empty:
        raise ValueError("æœªæŸ¥è¯¢åˆ°å¹´æŠ¥æ•°æ®ï¼Œè¯·ç¡®è®¤å…¬å¸æ˜¯å¦æŠ«éœ²å¹´æŠ¥ã€‚")

    # é‡è¦è°ƒè¯•ï¼šæ‰“å°æ—¥æœŸèŒƒå›´è¿‡æ»¤å‰çš„æ•°æ®
    if start_date or end_date:
        before_date_filter = len(df)
        before_date_years = sorted([row['end_date'][:4] for _, row in df.iterrows()]) if not df.empty else []
        print(f"ğŸ“… æ—¥æœŸèŒƒå›´è¿‡æ»¤å‰ï¼š{len(df)}æ¡è®°å½•ï¼Œå¹´ä»½ï¼š{before_date_years}")

    if start_date:
        df = df[df["end_date"] >= start_date]
    if end_date:
        df = df[df["end_date"] <= end_date]
    
    # é‡è¦è°ƒè¯•ï¼šæ‰“å°æ—¥æœŸèŒƒå›´è¿‡æ»¤åçš„æ•°æ®
    if start_date or end_date:
        after_date_years = sorted([row['end_date'][:4] for _, row in df.iterrows()]) if not df.empty else []
        print(f"ğŸ“… æ—¥æœŸèŒƒå›´è¿‡æ»¤åï¼š{len(df)}æ¡è®°å½•ï¼Œå¹´ä»½ï¼š{after_date_years}ï¼ˆèŒƒå›´ï¼š{start_date} - {end_date}ï¼‰")
    
    if df.empty:
        raise ValueError("æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰å¹´æŠ¥æ•°æ®ï¼Œè¯·è°ƒæ•´æ—¶é—´åŒºé—´ã€‚")

    df = df.sort_values("end_date", ascending=False)
    df = df.drop_duplicates(subset="end_date", keep="first")
    
    # é‡è¦ï¼šä¸é™åˆ¶æ•°æ®é‡ï¼Œè¿”å›æ‰€æœ‰ç¬¦åˆæ—¥æœŸèŒƒå›´çš„æ•°æ®
    # å·²ç»é€šè¿‡start_dateå’Œend_dateæ­£ç¡®è¿‡æ»¤äº†ï¼Œç›´æ¥è¿”å›æ‰€æœ‰æ•°æ®
    print(f"âœ… è·å–åˆ°{len(df)}æ¡å¹´åº¦æ•°æ®ï¼ˆä¸é™åˆ¶ï¼Œæ˜¾ç¤ºæ‰€æœ‰æ•°æ®ï¼‰")
    df = df.copy()

    for col in value_columns:
        df[col] = pd.to_numeric(df[col], errors="coerce")

    df = df.dropna(subset=value_columns, how="all")
    return df
